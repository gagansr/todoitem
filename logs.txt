* 
* ==> Audit <==
* |---------|------------------|----------|-------------|---------|---------------------|---------------------|
| Command |       Args       | Profile  |    User     | Version |     Start Time      |      End Time       |
|---------|------------------|----------|-------------|---------|---------------------|---------------------|
| start   |                  | minikube | gagansharma | v1.31.2 | 06 Nov 23 12:49 IST |                     |
| start   |                  | minikube | gagansharma | v1.31.2 | 06 Nov 23 12:50 IST | 06 Nov 23 13:02 IST |
| stop    |                  | minikube | gagansharma | v1.31.2 | 06 Nov 23 13:15 IST | 06 Nov 23 13:16 IST |
| start   |                  | minikube | gagansharma | v1.31.2 | 13 Nov 23 12:40 IST | 13 Nov 23 12:41 IST |
| start   |                  | minikube | gagansharma | v1.31.2 | 19 Nov 23 10:08 IST | 19 Nov 23 10:09 IST |
| start   |                  | minikube | gagansharma | v1.31.2 | 20 Nov 23 09:52 IST |                     |
| start   |                  | minikube | gagansharma | v1.31.2 | 25 Feb 24 09:44 IST | 25 Feb 24 09:47 IST |
| service | todoitem-service | minikube | gagansharma | v1.31.2 | 25 Feb 24 09:49 IST |                     |
| service | todoitem-service | minikube | gagansharma | v1.31.2 | 25 Feb 24 09:52 IST |                     |
|---------|------------------|----------|-------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/02/25 09:44:13
Running on machine: Gagans-MacBook-Air
Binary: Built with gc go1.21.0 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0225 09:44:13.234437    3595 out.go:296] Setting OutFile to fd 1 ...
I0225 09:44:13.234813    3595 out.go:348] isatty.IsTerminal(1) = true
I0225 09:44:13.234819    3595 out.go:309] Setting ErrFile to fd 2...
I0225 09:44:13.234829    3595 out.go:348] isatty.IsTerminal(2) = true
I0225 09:44:13.235176    3595 root.go:338] Updating PATH: /Users/gagansharma/.minikube/bin
W0225 09:44:13.235419    3595 root.go:314] Error reading config file at /Users/gagansharma/.minikube/config/config.json: open /Users/gagansharma/.minikube/config/config.json: no such file or directory
I0225 09:44:13.237212    3595 out.go:303] Setting JSON to false
I0225 09:44:13.285129    3595 start.go:128] hostinfo: {"hostname":"Gagans-MacBook-Air.local","uptime":12882,"bootTime":1708821571,"procs":475,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"12.7.2","kernelVersion":"21.6.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"281ddfc6-650a-51f1-b4e9-b4fa94ad4f61"}
W0225 09:44:13.285356    3595 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0225 09:44:13.299086    3595 out.go:177] 😄  minikube v1.31.2 on Darwin 12.7.2
I0225 09:44:13.316036    3595 notify.go:220] Checking for updates...
I0225 09:44:13.319093    3595 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0225 09:44:13.319374    3595 driver.go:373] Setting default libvirt URI to qemu:///system
I0225 09:44:13.557697    3595 lock.go:35] WriteFile acquiring /Users/gagansharma/.minikube/last_update_check: {Name:mk6cb40f761862ab102c2fc3b7e90b54eaeb2457 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0225 09:44:13.570416    3595 out.go:177] 🎉  minikube 1.32.0 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.32.0
I0225 09:44:13.584025    3595 out.go:177] 💡  To disable this notice, run: 'minikube config set WantUpdateNotification false'

I0225 09:44:13.911550    3595 docker.go:121] docker version: linux-24.0.6:Docker Desktop 4.24.2 (124339)
I0225 09:44:13.911741    3595 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0225 09:44:14.899915    3595 info.go:266] docker info: {ID:9ebb9eda-30fc-47ca-8042-2eacf335a1a6 Containers:3 ContainersRunning:0 ContainersPaused:0 ContainersStopped:3 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:40 OomKillDisable:false NGoroutines:60 SystemTime:2024-02-25 04:14:14.865657941 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:9 KernelVersion:6.4.16-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4122099712 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/gagansharma/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:/Users/gagansharma/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.22.0-desktop.2] map[Name:dev Path:/Users/gagansharma/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/gagansharma/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:/Users/gagansharma/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.8] map[Name:sbom Path:/Users/gagansharma/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/Users/gagansharma/.docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/Users/gagansharma/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.7]] Warnings:<nil>}}
I0225 09:44:14.910579    3595 out.go:177] ✨  Using the docker driver based on existing profile
I0225 09:44:14.922182    3595 start.go:298] selected driver: docker
I0225 09:44:14.922202    3595 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0225 09:44:14.922344    3595 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0225 09:44:14.922547    3595 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0225 09:44:15.191875    3595 info.go:266] docker info: {ID:9ebb9eda-30fc-47ca-8042-2eacf335a1a6 Containers:3 ContainersRunning:0 ContainersPaused:0 ContainersStopped:3 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:40 OomKillDisable:false NGoroutines:60 SystemTime:2024-02-25 04:14:15.158303306 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:9 KernelVersion:6.4.16-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4122099712 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/gagansharma/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:/Users/gagansharma/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.22.0-desktop.2] map[Name:dev Path:/Users/gagansharma/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/gagansharma/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:/Users/gagansharma/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.8] map[Name:sbom Path:/Users/gagansharma/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/Users/gagansharma/.docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/Users/gagansharma/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.7]] Warnings:<nil>}}
I0225 09:44:15.192354    3595 cni.go:84] Creating CNI manager for ""
I0225 09:44:15.192374    3595 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0225 09:44:15.192392    3595 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0225 09:44:15.204796    3595 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I0225 09:44:15.225720    3595 cache.go:122] Beginning downloading kic base image for docker with docker
I0225 09:44:15.236377    3595 out.go:177] 🚜  Pulling base image ...
I0225 09:44:15.261092    3595 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I0225 09:44:15.261238    3595 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon
I0225 09:44:15.261412    3595 preload.go:148] Found local preload: /Users/gagansharma/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4
I0225 09:44:15.261443    3595 cache.go:57] Caching tarball of preloaded images
I0225 09:44:15.263422    3595 preload.go:174] Found /Users/gagansharma/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0225 09:44:15.263737    3595 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.4 on docker
I0225 09:44:15.264367    3595 profile.go:148] Saving config to /Users/gagansharma/.minikube/profiles/minikube/config.json ...
I0225 09:44:15.371685    3595 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 to local cache
I0225 09:44:15.372101    3595 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local cache directory
I0225 09:44:15.372204    3595 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local cache directory, skipping pull
I0225 09:44:15.372211    3595 image.go:105] gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 exists in cache, skipping pull
I0225 09:44:15.372223    3595 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 as a tarball
I0225 09:44:15.372227    3595 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 from local cache
I0225 09:45:19.363256    3595 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 from cached tarball
I0225 09:45:19.363599    3595 cache.go:195] Successfully downloaded all kic artifacts
I0225 09:45:19.363854    3595 start.go:365] acquiring machines lock for minikube: {Name:mkd05562949c58eeee30422c5ad7c54608376b62 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0225 09:45:19.364483    3595 start.go:369] acquired machines lock for "minikube" in 504.403µs
I0225 09:45:19.364563    3595 start.go:96] Skipping create...Using existing machine configuration
I0225 09:45:19.364581    3595 fix.go:54] fixHost starting: 
I0225 09:45:19.366626    3595 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0225 09:45:19.574758    3595 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0225 09:45:19.574859    3595 fix.go:102] recreateIfNeeded on minikube: state= err=unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0225 09:45:19.574970    3595 fix.go:107] machineExists: false. err=machine does not exist
I0225 09:45:19.606202    3595 out.go:177] 🤷  docker "minikube" container is missing, will recreate.
I0225 09:45:19.618675    3595 delete.go:124] DEMOLISHING minikube ...
I0225 09:45:19.618911    3595 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0225 09:45:19.773883    3595 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W0225 09:45:19.774014    3595 stop.go:75] unable to get state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0225 09:45:19.774047    3595 delete.go:128] stophost failed (probably ok): ssh power off: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0225 09:45:19.775162    3595 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0225 09:45:19.937383    3595 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0225 09:45:19.937512    3595 delete.go:82] Unable to get host status for minikube, assuming it has already been deleted: state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0225 09:45:19.937698    3595 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W0225 09:45:20.058035    3595 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I0225 09:45:20.058186    3595 kic.go:367] could not find the container minikube to remove it. will try anyways
I0225 09:45:20.058382    3595 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0225 09:45:20.181608    3595 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W0225 09:45:20.181683    3595 oci.go:84] error getting container status, will try to delete anyways: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0225 09:45:20.181858    3595 cli_runner.go:164] Run: docker exec --privileged -t minikube /bin/bash -c "sudo init 0"
W0225 09:45:20.312044    3595 cli_runner.go:211] docker exec --privileged -t minikube /bin/bash -c "sudo init 0" returned with exit code 1
I0225 09:45:20.312086    3595 oci.go:647] error shutdown minikube: docker exec --privileged -t minikube /bin/bash -c "sudo init 0": exit status 1
stdout:

stderr:
Error response from daemon: No such container: minikube
I0225 09:45:21.313011    3595 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0225 09:45:21.449055    3595 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0225 09:45:21.449150    3595 oci.go:659] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0225 09:45:21.449169    3595 oci.go:661] temporary error: container minikube status is  but expect it to be exited
I0225 09:45:21.449275    3595 retry.go:31] will retry after 512.700715ms: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0225 09:45:21.962786    3595 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0225 09:45:22.074337    3595 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0225 09:45:22.074447    3595 oci.go:659] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0225 09:45:22.074487    3595 oci.go:661] temporary error: container minikube status is  but expect it to be exited
I0225 09:45:22.074541    3595 retry.go:31] will retry after 721.996623ms: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0225 09:45:22.797551    3595 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0225 09:45:22.913234    3595 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0225 09:45:22.913349    3595 oci.go:659] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0225 09:45:22.913373    3595 oci.go:661] temporary error: container minikube status is  but expect it to be exited
I0225 09:45:22.913407    3595 retry.go:31] will retry after 904.741487ms: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0225 09:45:23.818443    3595 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0225 09:45:23.960125    3595 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0225 09:45:23.960306    3595 oci.go:659] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0225 09:45:23.960364    3595 oci.go:661] temporary error: container minikube status is  but expect it to be exited
I0225 09:45:23.960409    3595 retry.go:31] will retry after 1.415158817s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0225 09:45:25.376595    3595 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0225 09:45:25.540321    3595 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0225 09:45:25.540469    3595 oci.go:659] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0225 09:45:25.540506    3595 oci.go:661] temporary error: container minikube status is  but expect it to be exited
I0225 09:45:25.540540    3595 retry.go:31] will retry after 2.322339457s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0225 09:45:27.863439    3595 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0225 09:45:28.615835    3595 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0225 09:45:28.615949    3595 oci.go:659] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0225 09:45:28.615967    3595 oci.go:661] temporary error: container minikube status is  but expect it to be exited
I0225 09:45:28.616028    3595 retry.go:31] will retry after 5.288991774s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0225 09:45:33.905526    3595 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0225 09:45:34.059972    3595 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0225 09:45:34.060134    3595 oci.go:659] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0225 09:45:34.060149    3595 oci.go:661] temporary error: container minikube status is  but expect it to be exited
I0225 09:45:34.060222    3595 retry.go:31] will retry after 6.313771105s: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0225 09:45:40.374808    3595 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0225 09:45:40.624707    3595 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0225 09:45:40.624886    3595 oci.go:659] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0225 09:45:40.624909    3595 oci.go:661] temporary error: container minikube status is  but expect it to be exited
I0225 09:45:40.624996    3595 oci.go:88] couldn't shut down minikube (might be okay): verify shutdown: couldn't verify container is exited. %!v(MISSING): unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
 
I0225 09:45:40.625252    3595 cli_runner.go:164] Run: docker rm -f -v minikube
I0225 09:45:40.811334    3595 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W0225 09:45:40.935466    3595 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I0225 09:45:40.935784    3595 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0225 09:45:41.114214    3595 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0225 09:45:41.114407    3595 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I0225 09:45:41.114433    3595 cli_runner.go:164] Run: docker network inspect minikube
W0225 09:45:41.281984    3595 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0225 09:45:41.282036    3595 network_create.go:284] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0225 09:45:41.282132    3595 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
W0225 09:45:41.282211    3595 network_create.go:311] Error inspecting docker network minikube: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}": exit status 1
stdout:


stderr:
Error response from daemon: network minikube not found
I0225 09:45:41.287016    3595 fix.go:114] Sleeping 1 second for extra luck!
I0225 09:45:42.287824    3595 start.go:125] createHost starting for "" (driver="docker")
I0225 09:45:42.303134    3595 out.go:204] 🔥  Creating docker container (CPUs=2, Memory=2200MB) ...
I0225 09:45:42.303710    3595 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0225 09:45:42.303782    3595 client.go:168] LocalClient.Create starting
I0225 09:45:42.304560    3595 main.go:141] libmachine: Reading certificate data from /Users/gagansharma/.minikube/certs/ca.pem
I0225 09:45:42.307937    3595 main.go:141] libmachine: Decoding PEM data...
I0225 09:45:42.307988    3595 main.go:141] libmachine: Parsing certificate...
I0225 09:45:42.308220    3595 main.go:141] libmachine: Reading certificate data from /Users/gagansharma/.minikube/certs/cert.pem
I0225 09:45:42.309701    3595 main.go:141] libmachine: Decoding PEM data...
I0225 09:45:42.309728    3595 main.go:141] libmachine: Parsing certificate...
I0225 09:45:42.310974    3595 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0225 09:45:42.493575    3595 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0225 09:45:42.493756    3595 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I0225 09:45:42.493795    3595 cli_runner.go:164] Run: docker network inspect minikube
W0225 09:45:42.686493    3595 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0225 09:45:42.686538    3595 network_create.go:284] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0225 09:45:42.686571    3595 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0225 09:45:42.686813    3595 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0225 09:45:42.857963    3595 network.go:209] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc000e373e0}
I0225 09:45:42.858084    3595 network_create.go:123] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0225 09:45:42.858298    3595 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0225 09:45:43.176432    3595 network_create.go:107] docker network minikube 192.168.49.0/24 created
I0225 09:45:43.176486    3595 kic.go:117] calculated static IP "192.168.49.2" for the "minikube" container
I0225 09:45:43.176734    3595 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0225 09:45:43.361584    3595 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0225 09:45:43.527516    3595 oci.go:103] Successfully created a docker volume minikube
I0225 09:45:43.527870    3595 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 -d /var/lib
I0225 09:45:46.409843    3595 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 -d /var/lib: (2.881741407s)
I0225 09:45:46.409881    3595 oci.go:107] Successfully prepared a docker volume minikube
I0225 09:45:46.409907    3595 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I0225 09:45:46.410027    3595 kic.go:190] Starting extracting preloaded images to volume ...
I0225 09:45:46.410317    3595 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/gagansharma/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 -I lz4 -xf /preloaded.tar -C /extractDir
I0225 09:46:04.848118    3595 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/gagansharma/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 -I lz4 -xf /preloaded.tar -C /extractDir: (18.436054881s)
I0225 09:46:04.848299    3595 kic.go:199] duration metric: took 18.437723 seconds to extract preloaded images to volume
I0225 09:46:04.848931    3595 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0225 09:46:06.702911    3595 cli_runner.go:217] Completed: docker info --format "'{{json .SecurityOptions}}'": (1.853833585s)
I0225 09:46:06.703386    3595 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631
I0225 09:46:08.775371    3595 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631: (2.071587416s)
I0225 09:46:08.775691    3595 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0225 09:46:09.020045    3595 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0225 09:46:09.222653    3595 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0225 09:46:10.132282    3595 oci.go:144] the created container "minikube" has a running status.
I0225 09:46:10.132374    3595 kic.go:221] Creating ssh key for kic: /Users/gagansharma/.minikube/machines/minikube/id_rsa...
I0225 09:46:10.316598    3595 kic_runner.go:191] docker (temp): /Users/gagansharma/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0225 09:46:10.663022    3595 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0225 09:46:10.874332    3595 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0225 09:46:10.874362    3595 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0225 09:46:11.344776    3595 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0225 09:46:11.534892    3595 machine.go:88] provisioning docker machine ...
I0225 09:46:11.535257    3595 ubuntu.go:169] provisioning hostname "minikube"
I0225 09:46:11.535552    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:46:11.702083    3595 main.go:141] libmachine: Using SSH client type: native
I0225 09:46:11.703419    3595 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f82c0] 0x1003fafa0 <nil>  [] 0s} 127.0.0.1 51145 <nil> <nil>}
I0225 09:46:11.703465    3595 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0225 09:46:12.177082    3595 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0225 09:46:12.177353    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:46:12.365129    3595 main.go:141] libmachine: Using SSH client type: native
I0225 09:46:12.366039    3595 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f82c0] 0x1003fafa0 <nil>  [] 0s} 127.0.0.1 51145 <nil> <nil>}
I0225 09:46:12.366069    3595 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0225 09:46:12.811521    3595 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0225 09:46:12.811701    3595 ubuntu.go:175] set auth options {CertDir:/Users/gagansharma/.minikube CaCertPath:/Users/gagansharma/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/gagansharma/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/gagansharma/.minikube/machines/server.pem ServerKeyPath:/Users/gagansharma/.minikube/machines/server-key.pem ClientKeyPath:/Users/gagansharma/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/gagansharma/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/gagansharma/.minikube}
I0225 09:46:12.811794    3595 ubuntu.go:177] setting up certificates
I0225 09:46:12.811809    3595 provision.go:83] configureAuth start
I0225 09:46:12.811938    3595 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0225 09:46:13.095090    3595 provision.go:138] copyHostCerts
I0225 09:46:13.097272    3595 exec_runner.go:144] found /Users/gagansharma/.minikube/ca.pem, removing ...
I0225 09:46:13.097292    3595 exec_runner.go:203] rm: /Users/gagansharma/.minikube/ca.pem
I0225 09:46:13.097615    3595 exec_runner.go:151] cp: /Users/gagansharma/.minikube/certs/ca.pem --> /Users/gagansharma/.minikube/ca.pem (1090 bytes)
I0225 09:46:13.099445    3595 exec_runner.go:144] found /Users/gagansharma/.minikube/cert.pem, removing ...
I0225 09:46:13.099458    3595 exec_runner.go:203] rm: /Users/gagansharma/.minikube/cert.pem
I0225 09:46:13.099652    3595 exec_runner.go:151] cp: /Users/gagansharma/.minikube/certs/cert.pem --> /Users/gagansharma/.minikube/cert.pem (1135 bytes)
I0225 09:46:13.101241    3595 exec_runner.go:144] found /Users/gagansharma/.minikube/key.pem, removing ...
I0225 09:46:13.101254    3595 exec_runner.go:203] rm: /Users/gagansharma/.minikube/key.pem
I0225 09:46:13.101502    3595 exec_runner.go:151] cp: /Users/gagansharma/.minikube/certs/key.pem --> /Users/gagansharma/.minikube/key.pem (1679 bytes)
I0225 09:46:13.102713    3595 provision.go:112] generating server cert: /Users/gagansharma/.minikube/machines/server.pem ca-key=/Users/gagansharma/.minikube/certs/ca.pem private-key=/Users/gagansharma/.minikube/certs/ca-key.pem org=gagansharma.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0225 09:46:13.629206    3595 provision.go:172] copyRemoteCerts
I0225 09:46:13.629617    3595 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0225 09:46:13.629753    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:46:13.945001    3595 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51145 SSHKeyPath:/Users/gagansharma/.minikube/machines/minikube/id_rsa Username:docker}
I0225 09:46:14.156037    3595 ssh_runner.go:362] scp /Users/gagansharma/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1090 bytes)
I0225 09:46:14.286067    3595 ssh_runner.go:362] scp /Users/gagansharma/.minikube/machines/server.pem --> /etc/docker/server.pem (1216 bytes)
I0225 09:46:14.373094    3595 ssh_runner.go:362] scp /Users/gagansharma/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0225 09:46:14.456405    3595 provision.go:86] duration metric: configureAuth took 1.644511519s
I0225 09:46:14.456429    3595 ubuntu.go:193] setting minikube options for container-runtime
I0225 09:46:14.458065    3595 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0225 09:46:14.458272    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:46:14.659544    3595 main.go:141] libmachine: Using SSH client type: native
I0225 09:46:14.660437    3595 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f82c0] 0x1003fafa0 <nil>  [] 0s} 127.0.0.1 51145 <nil> <nil>}
I0225 09:46:14.660457    3595 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0225 09:46:14.899897    3595 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0225 09:46:14.899929    3595 ubuntu.go:71] root file system type: overlay
I0225 09:46:14.900187    3595 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0225 09:46:14.900348    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:46:15.086348    3595 main.go:141] libmachine: Using SSH client type: native
I0225 09:46:15.087125    3595 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f82c0] 0x1003fafa0 <nil>  [] 0s} 127.0.0.1 51145 <nil> <nil>}
I0225 09:46:15.087288    3595 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0225 09:46:15.377788    3595 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0225 09:46:15.378006    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:46:15.562245    3595 main.go:141] libmachine: Using SSH client type: native
I0225 09:46:15.563167    3595 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f82c0] 0x1003fafa0 <nil>  [] 0s} 127.0.0.1 51145 <nil> <nil>}
I0225 09:46:15.563209    3595 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0225 09:46:18.041898    3595 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2023-07-07 14:50:55.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-02-25 04:16:15.363858326 +0000
@@ -1,30 +1,32 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +34,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0225 09:46:18.041950    3595 machine.go:91] provisioned docker machine in 6.506795934s
I0225 09:46:18.041962    3595 client.go:171] LocalClient.Create took 35.737153642s
I0225 09:46:18.042011    3595 start.go:167] duration metric: libmachine.API.Create for "minikube" took 35.737296788s
I0225 09:46:18.042029    3595 start.go:300] post-start starting for "minikube" (driver="docker")
I0225 09:46:18.042042    3595 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0225 09:46:18.042814    3595 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0225 09:46:18.043435    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:46:18.177297    3595 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51145 SSHKeyPath:/Users/gagansharma/.minikube/machines/minikube/id_rsa Username:docker}
I0225 09:46:18.363261    3595 ssh_runner.go:195] Run: cat /etc/os-release
I0225 09:46:18.378342    3595 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0225 09:46:18.378406    3595 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0225 09:46:18.378417    3595 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0225 09:46:18.378424    3595 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I0225 09:46:18.378437    3595 filesync.go:126] Scanning /Users/gagansharma/.minikube/addons for local assets ...
I0225 09:46:18.378757    3595 filesync.go:126] Scanning /Users/gagansharma/.minikube/files for local assets ...
I0225 09:46:18.378890    3595 start.go:303] post-start completed in 336.843001ms
I0225 09:46:18.380230    3595 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0225 09:46:18.514390    3595 profile.go:148] Saving config to /Users/gagansharma/.minikube/profiles/minikube/config.json ...
I0225 09:46:18.515415    3595 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0225 09:46:18.515498    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:46:18.651087    3595 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51145 SSHKeyPath:/Users/gagansharma/.minikube/machines/minikube/id_rsa Username:docker}
I0225 09:46:18.794829    3595 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0225 09:46:18.810823    3595 start.go:128] duration metric: createHost completed in 36.521936385s
I0225 09:46:18.811122    3595 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0225 09:46:18.939428    3595 fix.go:128] unexpected machine state, will restart: <nil>
I0225 09:46:18.939471    3595 machine.go:88] provisioning docker machine ...
I0225 09:46:18.939517    3595 ubuntu.go:169] provisioning hostname "minikube"
I0225 09:46:18.939773    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:46:19.109339    3595 main.go:141] libmachine: Using SSH client type: native
I0225 09:46:19.110080    3595 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f82c0] 0x1003fafa0 <nil>  [] 0s} 127.0.0.1 51145 <nil> <nil>}
I0225 09:46:19.110091    3595 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0225 09:46:19.360177    3595 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0225 09:46:19.360332    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:46:19.497783    3595 main.go:141] libmachine: Using SSH client type: native
I0225 09:46:19.498558    3595 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f82c0] 0x1003fafa0 <nil>  [] 0s} 127.0.0.1 51145 <nil> <nil>}
I0225 09:46:19.498590    3595 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0225 09:46:19.709957    3595 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0225 09:46:19.709982    3595 ubuntu.go:175] set auth options {CertDir:/Users/gagansharma/.minikube CaCertPath:/Users/gagansharma/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/gagansharma/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/gagansharma/.minikube/machines/server.pem ServerKeyPath:/Users/gagansharma/.minikube/machines/server-key.pem ClientKeyPath:/Users/gagansharma/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/gagansharma/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/gagansharma/.minikube}
I0225 09:46:19.710004    3595 ubuntu.go:177] setting up certificates
I0225 09:46:19.710013    3595 provision.go:83] configureAuth start
I0225 09:46:19.710108    3595 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0225 09:46:19.868579    3595 provision.go:138] copyHostCerts
I0225 09:46:19.868943    3595 exec_runner.go:144] found /Users/gagansharma/.minikube/key.pem, removing ...
I0225 09:46:19.868958    3595 exec_runner.go:203] rm: /Users/gagansharma/.minikube/key.pem
I0225 09:46:19.869240    3595 exec_runner.go:151] cp: /Users/gagansharma/.minikube/certs/key.pem --> /Users/gagansharma/.minikube/key.pem (1679 bytes)
I0225 09:46:19.871151    3595 exec_runner.go:144] found /Users/gagansharma/.minikube/ca.pem, removing ...
I0225 09:46:19.871161    3595 exec_runner.go:203] rm: /Users/gagansharma/.minikube/ca.pem
I0225 09:46:19.871316    3595 exec_runner.go:151] cp: /Users/gagansharma/.minikube/certs/ca.pem --> /Users/gagansharma/.minikube/ca.pem (1090 bytes)
I0225 09:46:19.871881    3595 exec_runner.go:144] found /Users/gagansharma/.minikube/cert.pem, removing ...
I0225 09:46:19.871895    3595 exec_runner.go:203] rm: /Users/gagansharma/.minikube/cert.pem
I0225 09:46:19.872093    3595 exec_runner.go:151] cp: /Users/gagansharma/.minikube/certs/cert.pem --> /Users/gagansharma/.minikube/cert.pem (1135 bytes)
I0225 09:46:19.873236    3595 provision.go:112] generating server cert: /Users/gagansharma/.minikube/machines/server.pem ca-key=/Users/gagansharma/.minikube/certs/ca.pem private-key=/Users/gagansharma/.minikube/certs/ca-key.pem org=gagansharma.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0225 09:46:20.301449    3595 provision.go:172] copyRemoteCerts
I0225 09:46:20.301539    3595 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0225 09:46:20.301652    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:46:20.434425    3595 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51145 SSHKeyPath:/Users/gagansharma/.minikube/machines/minikube/id_rsa Username:docker}
I0225 09:46:20.603225    3595 ssh_runner.go:362] scp /Users/gagansharma/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1090 bytes)
I0225 09:46:20.681080    3595 ssh_runner.go:362] scp /Users/gagansharma/.minikube/machines/server.pem --> /etc/docker/server.pem (1212 bytes)
I0225 09:46:20.746536    3595 ssh_runner.go:362] scp /Users/gagansharma/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0225 09:46:20.819438    3595 provision.go:86] duration metric: configureAuth took 1.109378131s
I0225 09:46:20.819460    3595 ubuntu.go:193] setting minikube options for container-runtime
I0225 09:46:20.819797    3595 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0225 09:46:20.819925    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:46:20.960848    3595 main.go:141] libmachine: Using SSH client type: native
I0225 09:46:20.962579    3595 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f82c0] 0x1003fafa0 <nil>  [] 0s} 127.0.0.1 51145 <nil> <nil>}
I0225 09:46:20.962597    3595 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0225 09:46:21.189291    3595 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0225 09:46:21.189368    3595 ubuntu.go:71] root file system type: overlay
I0225 09:46:21.189745    3595 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0225 09:46:21.189941    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:46:21.428908    3595 main.go:141] libmachine: Using SSH client type: native
I0225 09:46:21.429730    3595 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f82c0] 0x1003fafa0 <nil>  [] 0s} 127.0.0.1 51145 <nil> <nil>}
I0225 09:46:21.429854    3595 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0225 09:46:21.685310    3595 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0225 09:46:21.685465    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:46:21.833831    3595 main.go:141] libmachine: Using SSH client type: native
I0225 09:46:21.834831    3595 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f82c0] 0x1003fafa0 <nil>  [] 0s} 127.0.0.1 51145 <nil> <nil>}
I0225 09:46:21.834871    3595 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0225 09:46:22.096347    3595 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0225 09:46:22.096392    3595 machine.go:91] provisioned docker machine in 3.156794729s
I0225 09:46:22.096410    3595 start.go:300] post-start starting for "minikube" (driver="docker")
I0225 09:46:22.096420    3595 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0225 09:46:22.096523    3595 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0225 09:46:22.096607    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:46:22.247959    3595 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51145 SSHKeyPath:/Users/gagansharma/.minikube/machines/minikube/id_rsa Username:docker}
I0225 09:46:22.404911    3595 ssh_runner.go:195] Run: cat /etc/os-release
I0225 09:46:22.419681    3595 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0225 09:46:22.419741    3595 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0225 09:46:22.419761    3595 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0225 09:46:22.419770    3595 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I0225 09:46:22.419782    3595 filesync.go:126] Scanning /Users/gagansharma/.minikube/addons for local assets ...
I0225 09:46:22.419981    3595 filesync.go:126] Scanning /Users/gagansharma/.minikube/files for local assets ...
I0225 09:46:22.420099    3595 start.go:303] post-start completed in 323.667995ms
I0225 09:46:22.420196    3595 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0225 09:46:22.420326    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:46:22.559776    3595 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51145 SSHKeyPath:/Users/gagansharma/.minikube/machines/minikube/id_rsa Username:docker}
I0225 09:46:22.708447    3595 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0225 09:46:22.723852    3595 fix.go:56] fixHost completed within 1m3.357454138s
I0225 09:46:22.723896    3595 start.go:83] releasing machines lock for "minikube", held for 1m3.357586567s
I0225 09:46:22.724095    3595 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0225 09:46:22.884976    3595 ssh_runner.go:195] Run: cat /version.json
I0225 09:46:22.885868    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:46:22.886900    3595 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0225 09:46:22.887354    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:46:23.185027    3595 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51145 SSHKeyPath:/Users/gagansharma/.minikube/machines/minikube/id_rsa Username:docker}
I0225 09:46:23.201132    3595 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51145 SSHKeyPath:/Users/gagansharma/.minikube/machines/minikube/id_rsa Username:docker}
I0225 09:46:23.357585    3595 ssh_runner.go:195] Run: systemctl --version
I0225 09:46:23.926953    3595 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.039946608s)
I0225 09:46:23.927228    3595 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0225 09:46:23.947238    3595 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0225 09:46:24.022464    3595 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0225 09:46:24.022624    3595 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0225 09:46:24.119133    3595 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0225 09:46:24.119152    3595 start.go:466] detecting cgroup driver to use...
I0225 09:46:24.119175    3595 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0225 09:46:24.119400    3595 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0225 09:46:24.166084    3595 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0225 09:46:24.196308    3595 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0225 09:46:24.230008    3595 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0225 09:46:24.230246    3595 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0225 09:46:24.299764    3595 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0225 09:46:24.349938    3595 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0225 09:46:24.379645    3595 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0225 09:46:24.413751    3595 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0225 09:46:24.445386    3595 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0225 09:46:24.491480    3595 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0225 09:46:24.531547    3595 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0225 09:46:24.630920    3595 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0225 09:46:24.859751    3595 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0225 09:46:25.069267    3595 start.go:466] detecting cgroup driver to use...
I0225 09:46:25.069335    3595 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0225 09:46:25.069452    3595 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0225 09:46:25.120043    3595 cruntime.go:276] skipping containerd shutdown because we are bound to it
I0225 09:46:25.120331    3595 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0225 09:46:25.183078    3595 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0225 09:46:25.286984    3595 ssh_runner.go:195] Run: which cri-dockerd
I0225 09:46:25.303375    3595 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0225 09:46:25.337889    3595 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0225 09:46:25.994210    3595 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0225 09:46:26.306432    3595 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0225 09:46:26.552733    3595 docker.go:535] configuring docker to use "cgroupfs" as cgroup driver...
I0225 09:46:26.552756    3595 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I0225 09:46:26.682464    3595 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0225 09:46:26.999067    3595 ssh_runner.go:195] Run: sudo systemctl restart docker
I0225 09:46:30.793601    3595 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.794276143s)
I0225 09:46:30.794092    3595 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0225 09:46:31.126530    3595 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0225 09:46:31.396870    3595 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0225 09:46:31.617987    3595 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0225 09:46:31.854638    3595 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0225 09:46:31.908023    3595 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0225 09:46:32.146440    3595 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0225 09:46:32.588338    3595 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0225 09:46:32.589772    3595 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0225 09:46:32.611079    3595 start.go:534] Will wait 60s for crictl version
I0225 09:46:32.611319    3595 ssh_runner.go:195] Run: which crictl
I0225 09:46:32.628353    3595 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0225 09:46:32.957530    3595 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1
I0225 09:46:32.957685    3595 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0225 09:46:33.116376    3595 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0225 09:46:33.270626    3595 out.go:204] 🐳  Preparing Kubernetes v1.27.4 on Docker 24.0.4 ...
I0225 09:46:33.270943    3595 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0225 09:46:33.670291    3595 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0225 09:46:33.671597    3595 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0225 09:46:33.685202    3595 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0225 09:46:33.721079    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0225 09:46:33.997097    3595 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I0225 09:46:33.997404    3595 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0225 09:46:34.115750    3595 docker.go:636] Got preloaded images: -- stdout --
<none>:<none>
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0225 09:46:34.115793    3595 docker.go:566] Images already preloaded, skipping extraction
I0225 09:46:34.116117    3595 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0225 09:46:34.322624    3595 docker.go:636] Got preloaded images: -- stdout --
<none>:<none>
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0225 09:46:34.322656    3595 cache_images.go:84] Images are preloaded, skipping loading
I0225 09:46:34.322808    3595 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0225 09:46:34.627586    3595 cni.go:84] Creating CNI manager for ""
I0225 09:46:34.627629    3595 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0225 09:46:34.627707    3595 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0225 09:46:34.627759    3595 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.27.4 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0225 09:46:34.628658    3595 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.4
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0225 09:46:34.629028    3595 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.4/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0225 09:46:34.629208    3595 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.4
I0225 09:46:34.661103    3595 binaries.go:44] Found k8s binaries, skipping transfer
I0225 09:46:34.661284    3595 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0225 09:46:34.687211    3595 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0225 09:46:34.772655    3595 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0225 09:46:34.853022    3595 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0225 09:46:34.958648    3595 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0225 09:46:34.975294    3595 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0225 09:46:35.009442    3595 certs.go:56] Setting up /Users/gagansharma/.minikube/profiles/minikube for IP: 192.168.49.2
I0225 09:46:35.009479    3595 certs.go:190] acquiring lock for shared ca certs: {Name:mk6b22499ed2789c3db18162d686b14e10fab56f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0225 09:46:35.011768    3595 certs.go:199] skipping minikubeCA CA generation: /Users/gagansharma/.minikube/ca.key
I0225 09:46:35.013489    3595 certs.go:199] skipping proxyClientCA CA generation: /Users/gagansharma/.minikube/proxy-client-ca.key
I0225 09:46:35.015223    3595 certs.go:315] skipping minikube-user signed cert generation: /Users/gagansharma/.minikube/profiles/minikube/client.key
I0225 09:46:35.016842    3595 certs.go:315] skipping minikube signed cert generation: /Users/gagansharma/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0225 09:46:35.017384    3595 certs.go:315] skipping aggregator signed cert generation: /Users/gagansharma/.minikube/profiles/minikube/proxy-client.key
I0225 09:46:35.017882    3595 certs.go:437] found cert: /Users/gagansharma/.minikube/certs/Users/gagansharma/.minikube/certs/ca-key.pem (1679 bytes)
I0225 09:46:35.019119    3595 certs.go:437] found cert: /Users/gagansharma/.minikube/certs/Users/gagansharma/.minikube/certs/ca.pem (1090 bytes)
I0225 09:46:35.019413    3595 certs.go:437] found cert: /Users/gagansharma/.minikube/certs/Users/gagansharma/.minikube/certs/cert.pem (1135 bytes)
I0225 09:46:35.019721    3595 certs.go:437] found cert: /Users/gagansharma/.minikube/certs/Users/gagansharma/.minikube/certs/key.pem (1679 bytes)
I0225 09:46:35.022245    3595 ssh_runner.go:362] scp /Users/gagansharma/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0225 09:46:35.092918    3595 ssh_runner.go:362] scp /Users/gagansharma/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0225 09:46:35.162871    3595 ssh_runner.go:362] scp /Users/gagansharma/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0225 09:46:35.232910    3595 ssh_runner.go:362] scp /Users/gagansharma/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0225 09:46:35.300019    3595 ssh_runner.go:362] scp /Users/gagansharma/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0225 09:46:35.365105    3595 ssh_runner.go:362] scp /Users/gagansharma/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0225 09:46:35.495349    3595 ssh_runner.go:362] scp /Users/gagansharma/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0225 09:46:35.587947    3595 ssh_runner.go:362] scp /Users/gagansharma/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0225 09:46:36.404755    3595 ssh_runner.go:362] scp /Users/gagansharma/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0225 09:46:36.547909    3595 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0225 09:46:36.614284    3595 ssh_runner.go:195] Run: openssl version
I0225 09:46:36.646707    3595 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0225 09:46:36.688042    3595 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0225 09:46:36.705694    3595 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Nov  6 07:30 /usr/share/ca-certificates/minikubeCA.pem
I0225 09:46:36.705815    3595 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0225 09:46:36.744793    3595 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0225 09:46:36.803110    3595 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0225 09:46:36.816279    3595 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0225 09:46:36.841896    3595 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0225 09:46:36.859164    3595 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0225 09:46:36.884140    3595 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0225 09:46:36.912199    3595 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0225 09:46:36.936991    3595 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0225 09:46:36.968014    3595 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0225 09:46:36.968217    3595 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0225 09:46:37.061937    3595 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0225 09:46:37.092922    3595 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0225 09:46:37.092989    3595 kubeadm.go:636] restartCluster start
I0225 09:46:37.093088    3595 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0225 09:46:37.125408    3595 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0225 09:46:37.125674    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0225 09:46:37.355858    3595 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:49705"
I0225 09:46:37.355883    3595 kubeconfig.go:135] verify returned: got: 127.0.0.1:49705, want: 127.0.0.1:51149
I0225 09:46:37.357067    3595 lock.go:35] WriteFile acquiring /Users/gagansharma/.kube/config: {Name:mk649e7441c541ea9319ee6759ba75ba57510480 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0225 09:46:37.362957    3595 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0225 09:46:37.394458    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:37.394685    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:37.430896    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:37.430918    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:37.431025    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:37.464944    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:37.965139    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:37.965292    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:37.996262    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:38.466612    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:38.466777    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:38.512927    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:38.965163    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:38.965354    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:39.018966    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:39.465458    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:39.465608    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:39.505059    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:39.965540    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:39.965762    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:40.226235    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:40.576071    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:40.576213    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:40.765920    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:40.965494    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:40.965746    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:41.004198    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:41.465463    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:41.465754    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:41.534815    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:41.965519    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:41.965738    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:42.102887    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:42.466933    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:42.467798    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:42.767152    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:42.965276    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:42.965403    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:43.073018    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:43.465530    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:43.465721    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:43.508740    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:43.965318    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:43.967278    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:44.102748    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:44.465544    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:44.465767    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:44.538791    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:44.966374    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:44.966609    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:45.037331    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:45.465627    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:45.467064    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:45.611341    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:45.967342    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:45.967478    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:46.016165    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:46.466519    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:46.466679    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:46.519057    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:46.965439    3595 api_server.go:166] Checking apiserver status ...
I0225 09:46:46.965630    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0225 09:46:47.025498    3595 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0225 09:46:47.395428    3595 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0225 09:46:47.395532    3595 kubeadm.go:1128] stopping kube-system containers ...
I0225 09:46:47.395864    3595 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0225 09:46:47.532202    3595 docker.go:462] Stopping containers: [ed128904bd4c 29bc99a62968 6ad1871c9cd2 9ac136cc44cc 6b00674c8c09 da29bb2bcaab a5367f414126 54aa04a99ec2 ef15122f1653 41595d4aac9e ac6e6aa591ed 7798e0c7163a 95d54aad3308 57e143275b89 f3a6563e2e9f 0785880c827f ada8417ed94e a0ada6bdaa22 35f2ec46555c ad7ab2d49890 29511e2b0547 d4e3b229f8ab c72ca11bc1bf 9b612aad8002 aeac53a71364 600dae19278d 36c227dbb198 8e126342e59e]
I0225 09:46:47.532337    3595 ssh_runner.go:195] Run: docker stop ed128904bd4c 29bc99a62968 6ad1871c9cd2 9ac136cc44cc 6b00674c8c09 da29bb2bcaab a5367f414126 54aa04a99ec2 ef15122f1653 41595d4aac9e ac6e6aa591ed 7798e0c7163a 95d54aad3308 57e143275b89 f3a6563e2e9f 0785880c827f ada8417ed94e a0ada6bdaa22 35f2ec46555c ad7ab2d49890 29511e2b0547 d4e3b229f8ab c72ca11bc1bf 9b612aad8002 aeac53a71364 600dae19278d 36c227dbb198 8e126342e59e
I0225 09:46:47.654954    3595 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0225 09:46:47.725044    3595 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0225 09:46:47.761876    3595 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0225 09:46:47.762002    3595 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0225 09:46:47.805449    3595 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0225 09:46:47.805465    3595 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0225 09:46:49.320992    3595 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml": (1.515445163s)
I0225 09:46:49.321173    3595 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0225 09:46:51.489784    3595 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (2.168490876s)
I0225 09:46:51.489850    3595 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0225 09:46:52.460066    3595 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0225 09:46:52.682764    3595 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0225 09:46:52.869966    3595 api_server.go:52] waiting for apiserver process to appear ...
I0225 09:46:52.870943    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:46:52.920084    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:46:53.456996    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:46:53.957017    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:46:54.456939    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:46:54.956873    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:46:55.457265    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:46:55.956831    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:46:56.457074    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:46:56.957031    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:46:57.458261    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:46:57.957255    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:46:58.456830    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:46:58.957986    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:46:59.457411    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:46:59.500188    3595 api_server.go:72] duration metric: took 6.630071024s to wait for apiserver process to appear ...
I0225 09:46:59.500228    3595 api_server.go:88] waiting for apiserver healthz status ...
I0225 09:46:59.500258    3595 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51149/healthz ...
I0225 09:46:59.508623    3595 api_server.go:269] stopped: https://127.0.0.1:51149/healthz: Get "https://127.0.0.1:51149/healthz": EOF
I0225 09:46:59.508665    3595 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51149/healthz ...
I0225 09:46:59.511812    3595 api_server.go:269] stopped: https://127.0.0.1:51149/healthz: Get "https://127.0.0.1:51149/healthz": EOF
I0225 09:47:00.013205    3595 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51149/healthz ...
I0225 09:47:00.017847    3595 api_server.go:269] stopped: https://127.0.0.1:51149/healthz: Get "https://127.0.0.1:51149/healthz": EOF
I0225 09:47:00.523066    3595 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51149/healthz ...
I0225 09:47:00.525804    3595 api_server.go:269] stopped: https://127.0.0.1:51149/healthz: Get "https://127.0.0.1:51149/healthz": EOF
I0225 09:47:01.012979    3595 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51149/healthz ...
I0225 09:47:04.379639    3595 api_server.go:279] https://127.0.0.1:51149/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0225 09:47:04.379660    3595 api_server.go:103] status: https://127.0.0.1:51149/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0225 09:47:04.379682    3595 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51149/healthz ...
I0225 09:47:04.743892    3595 api_server.go:279] https://127.0.0.1:51149/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0225 09:47:04.743917    3595 api_server.go:103] status: https://127.0.0.1:51149/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0225 09:47:04.743937    3595 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51149/healthz ...
I0225 09:47:05.021612    3595 api_server.go:279] https://127.0.0.1:51149/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0225 09:47:05.021649    3595 api_server.go:103] status: https://127.0.0.1:51149/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0225 09:47:05.021678    3595 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51149/healthz ...
I0225 09:47:05.149519    3595 api_server.go:279] https://127.0.0.1:51149/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
W0225 09:47:05.149562    3595 api_server.go:103] status: https://127.0.0.1:51149/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
I0225 09:47:05.512328    3595 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51149/healthz ...
I0225 09:47:05.597200    3595 api_server.go:279] https://127.0.0.1:51149/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0225 09:47:05.597243    3595 api_server.go:103] status: https://127.0.0.1:51149/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0225 09:47:06.013150    3595 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51149/healthz ...
I0225 09:47:06.134309    3595 api_server.go:279] https://127.0.0.1:51149/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0225 09:47:06.134351    3595 api_server.go:103] status: https://127.0.0.1:51149/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0225 09:47:06.512328    3595 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51149/healthz ...
I0225 09:47:06.588369    3595 api_server.go:279] https://127.0.0.1:51149/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0225 09:47:06.588405    3595 api_server.go:103] status: https://127.0.0.1:51149/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0225 09:47:07.012759    3595 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51149/healthz ...
I0225 09:47:07.028773    3595 api_server.go:279] https://127.0.0.1:51149/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0225 09:47:07.028821    3595 api_server.go:103] status: https://127.0.0.1:51149/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0225 09:47:07.512281    3595 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51149/healthz ...
I0225 09:47:07.581364    3595 api_server.go:279] https://127.0.0.1:51149/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0225 09:47:07.581407    3595 api_server.go:103] status: https://127.0.0.1:51149/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0225 09:47:08.012475    3595 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51149/healthz ...
I0225 09:47:08.096913    3595 api_server.go:279] https://127.0.0.1:51149/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0225 09:47:08.096944    3595 api_server.go:103] status: https://127.0.0.1:51149/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0225 09:47:08.512265    3595 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51149/healthz ...
I0225 09:47:08.571634    3595 api_server.go:279] https://127.0.0.1:51149/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0225 09:47:08.571668    3595 api_server.go:103] status: https://127.0.0.1:51149/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0225 09:47:09.012519    3595 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51149/healthz ...
I0225 09:47:09.033403    3595 api_server.go:279] https://127.0.0.1:51149/healthz returned 200:
ok
I0225 09:47:09.066350    3595 api_server.go:141] control plane version: v1.27.4
I0225 09:47:09.066396    3595 api_server.go:131] duration metric: took 9.565867562s to wait for apiserver health ...
I0225 09:47:09.066453    3595 cni.go:84] Creating CNI manager for ""
I0225 09:47:09.066789    3595 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0225 09:47:09.083361    3595 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0225 09:47:09.096102    3595 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0225 09:47:09.132782    3595 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0225 09:47:09.201407    3595 system_pods.go:43] waiting for kube-system pods to appear ...
I0225 09:47:09.247631    3595 system_pods.go:59] 7 kube-system pods found
I0225 09:47:09.247668    3595 system_pods.go:61] "coredns-5d78c9869d-46l2d" [696977d6-e0ed-44bf-a4cd-64e19362a243] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0225 09:47:09.247682    3595 system_pods.go:61] "etcd-minikube" [e3f0ef00-6a75-43d2-80db-9552f73cb567] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0225 09:47:09.247693    3595 system_pods.go:61] "kube-apiserver-minikube" [7bff6120-0ed3-4ed3-ac3e-bdc1ad77486e] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0225 09:47:09.247703    3595 system_pods.go:61] "kube-controller-manager-minikube" [298d51a6-4f05-487b-a6e0-b8f22531643d] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0225 09:47:09.247713    3595 system_pods.go:61] "kube-proxy-wbb49" [fcb8012d-ea90-44a5-8010-c069792791c3] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0225 09:47:09.247722    3595 system_pods.go:61] "kube-scheduler-minikube" [30ed51f3-74f2-4f46-b6ae-e91b5af3d628] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0225 09:47:09.247729    3595 system_pods.go:61] "storage-provisioner" [0b3c0c07-ae4e-42cc-92d3-fb22431952f8] Running
I0225 09:47:09.247738    3595 system_pods.go:74] duration metric: took 46.317693ms to wait for pod list to return data ...
I0225 09:47:09.247748    3595 node_conditions.go:102] verifying NodePressure condition ...
I0225 09:47:09.277291    3595 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0225 09:47:09.277314    3595 node_conditions.go:123] node cpu capacity is 4
I0225 09:47:09.277337    3595 node_conditions.go:105] duration metric: took 29.582363ms to run NodePressure ...
I0225 09:47:09.277371    3595 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0225 09:47:10.443377    3595 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (1.165934403s)
I0225 09:47:10.443442    3595 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0225 09:47:10.479067    3595 ops.go:34] apiserver oom_adj: -16
I0225 09:47:10.479082    3595 kubeadm.go:640] restartCluster took 33.385129308s
I0225 09:47:10.479104    3595 kubeadm.go:406] StartCluster complete in 33.510149673s
I0225 09:47:10.479152    3595 settings.go:142] acquiring lock: {Name:mkdf07e8c05840620c9c5a0f4297fb1e3cb7a5a5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0225 09:47:10.481499    3595 settings.go:150] Updating kubeconfig:  /Users/gagansharma/.kube/config
I0225 09:47:10.486930    3595 lock.go:35] WriteFile acquiring /Users/gagansharma/.kube/config: {Name:mk649e7441c541ea9319ee6759ba75ba57510480 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0225 09:47:10.487786    3595 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0225 09:47:10.488895    3595 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0225 09:47:10.488831    3595 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I0225 09:47:10.488940    3595 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0225 09:47:10.488948    3595 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0225 09:47:10.488961    3595 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0225 09:47:10.488963    3595 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0225 09:47:10.488970    3595 addons.go:240] addon storage-provisioner should already be in state true
I0225 09:47:10.489030    3595 host.go:66] Checking if "minikube" exists ...
I0225 09:47:10.492393    3595 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0225 09:47:10.492393    3595 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0225 09:47:10.520815    3595 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0225 09:47:10.520886    3595 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}
I0225 09:47:10.534700    3595 out.go:177] 🔎  Verifying Kubernetes components...
I0225 09:47:10.594468    3595 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0225 09:47:11.388020    3595 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0225 09:47:11.407983    3595 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0225 09:47:11.408005    3595 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0225 09:47:11.408233    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:47:11.472504    3595 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0225 09:47:11.472533    3595 addons.go:240] addon default-storageclass should already be in state true
I0225 09:47:11.472566    3595 host.go:66] Checking if "minikube" exists ...
I0225 09:47:11.474001    3595 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0225 09:47:11.629822    3595 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51145 SSHKeyPath:/Users/gagansharma/.minikube/machines/minikube/id_rsa Username:docker}
I0225 09:47:11.679989    3595 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0225 09:47:11.680004    3595 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0225 09:47:11.680156    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0225 09:47:11.846156    3595 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51145 SSHKeyPath:/Users/gagansharma/.minikube/machines/minikube/id_rsa Username:docker}
I0225 09:47:12.047552    3595 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0225 09:47:12.189932    3595 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0225 09:47:12.768859    3595 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (2.280972243s)
I0225 09:47:12.769039    3595 start.go:874] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0225 09:47:12.769090    3595 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (2.174520155s)
I0225 09:47:12.769395    3595 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0225 09:47:12.932568    3595 api_server.go:52] waiting for apiserver process to appear ...
I0225 09:47:12.932695    3595 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0225 09:47:14.606982    3595 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.559307662s)
I0225 09:47:14.607089    3595 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.417052187s)
I0225 09:47:14.607114    3595 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.674358627s)
I0225 09:47:14.607131    3595 api_server.go:72] duration metric: took 4.08606389s to wait for apiserver process to appear ...
I0225 09:47:14.607140    3595 api_server.go:88] waiting for apiserver healthz status ...
I0225 09:47:14.607158    3595 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51149/healthz ...
I0225 09:47:14.621392    3595 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0225 09:47:14.633000    3595 addons.go:502] enable addons completed in 4.144069712s: enabled=[storage-provisioner default-storageclass]
I0225 09:47:14.621971    3595 api_server.go:279] https://127.0.0.1:51149/healthz returned 200:
ok
I0225 09:47:14.638025    3595 api_server.go:141] control plane version: v1.27.4
I0225 09:47:14.638042    3595 api_server.go:131] duration metric: took 30.89374ms to wait for apiserver health ...
I0225 09:47:14.638050    3595 system_pods.go:43] waiting for kube-system pods to appear ...
I0225 09:47:14.666549    3595 system_pods.go:59] 7 kube-system pods found
I0225 09:47:14.666568    3595 system_pods.go:61] "coredns-5d78c9869d-46l2d" [696977d6-e0ed-44bf-a4cd-64e19362a243] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0225 09:47:14.666575    3595 system_pods.go:61] "etcd-minikube" [e3f0ef00-6a75-43d2-80db-9552f73cb567] Running
I0225 09:47:14.666580    3595 system_pods.go:61] "kube-apiserver-minikube" [7bff6120-0ed3-4ed3-ac3e-bdc1ad77486e] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0225 09:47:14.666593    3595 system_pods.go:61] "kube-controller-manager-minikube" [298d51a6-4f05-487b-a6e0-b8f22531643d] Running
I0225 09:47:14.666597    3595 system_pods.go:61] "kube-proxy-wbb49" [fcb8012d-ea90-44a5-8010-c069792791c3] Running
I0225 09:47:14.666601    3595 system_pods.go:61] "kube-scheduler-minikube" [30ed51f3-74f2-4f46-b6ae-e91b5af3d628] Running
I0225 09:47:14.666604    3595 system_pods.go:61] "storage-provisioner" [0b3c0c07-ae4e-42cc-92d3-fb22431952f8] Running
I0225 09:47:14.666609    3595 system_pods.go:74] duration metric: took 28.553112ms to wait for pod list to return data ...
I0225 09:47:14.666615    3595 kubeadm.go:581] duration metric: took 4.145552177s to wait for : map[apiserver:true system_pods:true] ...
I0225 09:47:14.666625    3595 node_conditions.go:102] verifying NodePressure condition ...
I0225 09:47:14.673737    3595 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0225 09:47:14.673749    3595 node_conditions.go:123] node cpu capacity is 4
I0225 09:47:14.673777    3595 node_conditions.go:105] duration metric: took 7.144808ms to run NodePressure ...
I0225 09:47:14.673793    3595 start.go:228] waiting for startup goroutines ...
I0225 09:47:14.673806    3595 start.go:233] waiting for cluster config update ...
I0225 09:47:14.673839    3595 start.go:242] writing updated cluster config ...
I0225 09:47:14.674791    3595 ssh_runner.go:195] Run: rm -f paused
I0225 09:47:14.773698    3595 start.go:600] kubectl: 1.28.3, cluster: 1.27.4 (minor skew: 1)
I0225 09:47:14.787998    3595 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Feb 25 04:17:09 minikube cri-dockerd[1439]: time="2024-02-25T04:17:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-46l2d_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Feb 25 04:17:09 minikube cri-dockerd[1439]: time="2024-02-25T04:17:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-77b4fdf86c-vnjmc_default\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Feb 25 04:17:09 minikube dockerd[1205]: time="2024-02-25T04:17:09.795778103Z" level=info msg="ignoring event" container=eea4129803fb92dd663b74480263470a2b63a38b40eb60e6742b7bd445e9b241 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 25 04:17:10 minikube dockerd[1205]: time="2024-02-25T04:17:10.105503016Z" level=info msg="ignoring event" container=4af0713e13187631b966436a7e58e06c92bf4577d727b160a3d8b00d6b33b7d6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 25 04:17:11 minikube cri-dockerd[1439]: time="2024-02-25T04:17:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0b667b18bfbb79ab12c4f570e6580544ab4b90567695235b0a5a997a103fd365/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 25 04:17:11 minikube cri-dockerd[1439]: time="2024-02-25T04:17:11Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-46l2d_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Feb 25 04:17:12 minikube cri-dockerd[1439]: time="2024-02-25T04:17:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-46l2d_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Feb 25 04:17:22 minikube cri-dockerd[1439]: time="2024-02-25T04:17:22Z" level=info msg="Pulling image nginx:latest: 88f6f236f401: Downloading [===============================>                   ]  25.93MB/41.39MB"
Feb 25 04:17:26 minikube dockerd[1205]: time="2024-02-25T04:17:26.914825467Z" level=info msg="ignoring event" container=9cd8535eb3e65edfb5f1124cde2ef67589e472d7eeb2823a7caa533f23cbc526 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 25 04:17:27 minikube dockerd[1205]: time="2024-02-25T04:17:27.430213082Z" level=info msg="ignoring event" container=0b667b18bfbb79ab12c4f570e6580544ab4b90567695235b0a5a997a103fd365 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 25 04:17:28 minikube cri-dockerd[1439]: time="2024-02-25T04:17:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4c377674447f07ea94e0cb0b0281c73b1bee7cecb2c557b830d342bea2e7a60d/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 25 04:17:32 minikube cri-dockerd[1439]: time="2024-02-25T04:17:32Z" level=info msg="Pulling image nginx:latest: e1caac4eb9d2: Extracting [================================================>  ]  28.02MB/29.12MB"
Feb 25 04:17:38 minikube dockerd[1205]: time="2024-02-25T04:17:38.359630969Z" level=info msg="ignoring event" container=633338e3214129917969785699acd9c585944aee0831c341b900a40a7b282ee9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 25 04:17:40 minikube cri-dockerd[1439]: time="2024-02-25T04:17:40Z" level=error msg="Error response from daemon: No such container: 29bc99a62968359d39646e3ea7c118998f21b31000b3a10bb98e38ba8babf50c Failed to get stats from container 29bc99a62968359d39646e3ea7c118998f21b31000b3a10bb98e38ba8babf50c"
Feb 25 04:17:42 minikube cri-dockerd[1439]: time="2024-02-25T04:17:42Z" level=info msg="Pulling image nginx:latest: 88f6f236f401: Extracting [==========================================>        ]  35.36MB/41.39MB"
Feb 25 04:17:45 minikube cri-dockerd[1439]: time="2024-02-25T04:17:45Z" level=info msg="Stop pulling image nginx:latest: Status: Downloaded newer image for nginx:latest"
Feb 25 04:17:46 minikube cri-dockerd[1439]: time="2024-02-25T04:17:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/60d5f5e02db18a468da6a21df87044114a45ce177ad183948339b55ff4ff907c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 25 04:17:46 minikube cri-dockerd[1439]: time="2024-02-25T04:17:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0b54026eb673e40db85849ce3be9b9e829147603aaebc850ac23e2f953c2288d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 25 04:17:46 minikube cri-dockerd[1439]: time="2024-02-25T04:17:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e27538c71d7dd4b5c0398d439e277251254e4f1c0ac1394e904bdbd99b58b1a6/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 25 04:17:47 minikube cri-dockerd[1439]: time="2024-02-25T04:17:47Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-77b4fdf86c-vnjmc_default\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Feb 25 04:17:48 minikube dockerd[1205]: time="2024-02-25T04:17:48.392790514Z" level=info msg="ignoring event" container=0ec51d342b2f6ab3b6ba9059685e9a957bb6839d4226158e95c90df17409022a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 25 04:17:48 minikube dockerd[1205]: time="2024-02-25T04:17:48.553015027Z" level=info msg="ignoring event" container=4576ee03efc5600a769dab20ce05fd5e198f123dd70d1ee693fd5ef98a037343 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 25 04:17:48 minikube cri-dockerd[1439]: time="2024-02-25T04:17:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/87ae57cd520769d269a05fe6891c74b74e0ac78a51302df1427b565ed68742cf/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 25 04:17:50 minikube dockerd[1205]: time="2024-02-25T04:17:50.468619262Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 25 04:17:50 minikube dockerd[1205]: time="2024-02-25T04:17:50.468812958Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 25 04:17:54 minikube dockerd[1205]: time="2024-02-25T04:17:54.329551610Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 25 04:17:54 minikube dockerd[1205]: time="2024-02-25T04:17:54.329774151Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 25 04:17:58 minikube dockerd[1205]: time="2024-02-25T04:17:58.116801064Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 25 04:17:58 minikube dockerd[1205]: time="2024-02-25T04:17:58.116892910Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 25 04:18:01 minikube cri-dockerd[1439]: time="2024-02-25T04:18:01Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Feb 25 04:18:06 minikube dockerd[1205]: time="2024-02-25T04:18:06.085435120Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 25 04:18:06 minikube dockerd[1205]: time="2024-02-25T04:18:06.085974256Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 25 04:18:12 minikube dockerd[1205]: time="2024-02-25T04:18:12.259305783Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 25 04:18:12 minikube dockerd[1205]: time="2024-02-25T04:18:12.259514894Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 25 04:18:16 minikube dockerd[1205]: time="2024-02-25T04:18:16.356319027Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 25 04:18:16 minikube dockerd[1205]: time="2024-02-25T04:18:16.356459837Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 25 04:18:38 minikube dockerd[1205]: time="2024-02-25T04:18:38.268361440Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 25 04:18:38 minikube dockerd[1205]: time="2024-02-25T04:18:38.268499790Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 25 04:18:46 minikube dockerd[1205]: time="2024-02-25T04:18:46.254102507Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 25 04:18:46 minikube dockerd[1205]: time="2024-02-25T04:18:46.254278768Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 25 04:18:52 minikube dockerd[1205]: time="2024-02-25T04:18:52.091935758Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 25 04:18:52 minikube dockerd[1205]: time="2024-02-25T04:18:52.092293641Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 25 04:19:31 minikube dockerd[1205]: time="2024-02-25T04:19:31.715268458Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 25 04:19:31 minikube dockerd[1205]: time="2024-02-25T04:19:31.715336056Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 25 04:19:35 minikube dockerd[1205]: time="2024-02-25T04:19:35.611821280Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 25 04:19:35 minikube dockerd[1205]: time="2024-02-25T04:19:35.612029967Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 25 04:19:41 minikube dockerd[1205]: time="2024-02-25T04:19:41.007440192Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 25 04:19:41 minikube dockerd[1205]: time="2024-02-25T04:19:41.007602130Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 25 04:20:58 minikube dockerd[1205]: time="2024-02-25T04:20:58.144330052Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 25 04:20:58 minikube dockerd[1205]: time="2024-02-25T04:20:58.145159336Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 25 04:21:13 minikube dockerd[1205]: time="2024-02-25T04:21:13.239965602Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 25 04:21:13 minikube dockerd[1205]: time="2024-02-25T04:21:13.240028119Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 25 04:21:17 minikube dockerd[1205]: time="2024-02-25T04:21:17.380711368Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 25 04:21:17 minikube dockerd[1205]: time="2024-02-25T04:21:17.380892891Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 25 04:23:45 minikube dockerd[1205]: time="2024-02-25T04:23:45.363393857Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 25 04:23:45 minikube dockerd[1205]: time="2024-02-25T04:23:45.363449049Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 25 04:24:03 minikube dockerd[1205]: time="2024-02-25T04:24:03.024662032Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 25 04:24:03 minikube dockerd[1205]: time="2024-02-25T04:24:03.025496072Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 25 04:24:11 minikube dockerd[1205]: time="2024-02-25T04:24:11.170418813Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 25 04:24:11 minikube dockerd[1205]: time="2024-02-25T04:24:11.170607232Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                           CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
4007cce36c112       nginx@sha256:c26ae7472d624ba1fafd296e73cecc4f93f853088e6a9c13c0d52f6ca5865107   6 minutes ago       Running             nginx                     2                   87ae57cd52076       nginx-77b4fdf86c-vnjmc
eeb7c5b910ac3       6e38f40d628db                                                                   6 minutes ago       Running             storage-provisioner       11                  5597a62fc6772       storage-provisioner
64b87ab0efa92       ead0a4a53df89                                                                   6 minutes ago       Running             coredns                   6                   4c377674447f0       coredns-5d78c9869d-46l2d
0ec51d342b2f6       nginx@sha256:c26ae7472d624ba1fafd296e73cecc4f93f853088e6a9c13c0d52f6ca5865107   6 minutes ago       Exited              nginx                     1                   4576ee03efc56       nginx-77b4fdf86c-vnjmc
9cd8535eb3e65       ead0a4a53df89                                                                   7 minutes ago       Exited              coredns                   5                   0b667b18bfbb7       coredns-5d78c9869d-46l2d
633338e321412       6e38f40d628db                                                                   7 minutes ago       Exited              storage-provisioner       10                  5597a62fc6772       storage-provisioner
f984c2fc30645       6848d7eda0341                                                                   7 minutes ago       Running             kube-proxy                4                   68b614b18306a       kube-proxy-wbb49
edd444775bf68       e7972205b6614                                                                   7 minutes ago       Running             kube-apiserver            4                   951f08364257f       kube-apiserver-minikube
2bbabf72d5755       86b6af7dd652c                                                                   7 minutes ago       Running             etcd                      4                   1ca4c1bd363be       etcd-minikube
58180cecd3e78       f466468864b7a                                                                   7 minutes ago       Running             kube-controller-manager   5                   c43a707ecb508       kube-controller-manager-minikube
a3dfbbf8c2251       98ef2570f3cde                                                                   7 minutes ago       Running             kube-scheduler            4                   cd6d581073115       kube-scheduler-minikube
6ad1871c9cd25       6848d7eda0341                                                                   3 months ago        Exited              kube-proxy                3                   9ac136cc44ccb       kube-proxy-wbb49
a5367f414126f       f466468864b7a                                                                   3 months ago        Exited              kube-controller-manager   4                   7798e0c7163a7       kube-controller-manager-minikube
54aa04a99ec20       98ef2570f3cde                                                                   3 months ago        Exited              kube-scheduler            3                   ac6e6aa591ed9       kube-scheduler-minikube
ef15122f16532       e7972205b6614                                                                   3 months ago        Exited              kube-apiserver            3                   95d54aad33086       kube-apiserver-minikube
41595d4aac9ef       86b6af7dd652c                                                                   3 months ago        Exited              etcd                      3                   57e143275b891       etcd-minikube

* 
* ==> coredns [64b87ab0efa9] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:60548 - 54307 "HINFO IN 1584500825934745980.1612166770003699911. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.465081143s

* 
* ==> coredns [9cd8535eb3e6] <==
* [INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] plugin/health: Going into lameduck mode for 5s
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: network is unreachable
[INFO] 127.0.0.1:44379 - 58724 "HINFO IN 2476766707570486255.7215227482303610893. udp 57 false 512" - - 0 5.000233942s
[ERROR] plugin/errors: 2 2476766707570486255.7215227482303610893. HINFO: dial udp 192.168.65.254:53: connect: network is unreachable
[INFO] 127.0.0.1:58695 - 20882 "HINFO IN 2476766707570486255.7215227482303610893. udp 57 false 512" - - 0 5.001533124s
[ERROR] plugin/errors: 2 2476766707570486255.7215227482303610893. HINFO: dial udp 192.168.65.254:53: connect: network is unreachable

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fd7ecd9c4599bef9f04c0986c4a0187f98a4396e
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_11_06T13_02_17_0700
                    minikube.k8s.io/version=v1.31.2
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 06 Nov 2023 07:32:07 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 25 Feb 2024 04:24:17 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 25 Feb 2024 04:23:05 +0000   Sun, 19 Nov 2023 15:25:10 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 25 Feb 2024 04:23:05 +0000   Sun, 19 Nov 2023 15:25:10 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 25 Feb 2024 04:23:05 +0000   Sun, 19 Nov 2023 15:25:10 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 25 Feb 2024 04:23:05 +0000   Sun, 19 Nov 2023 15:25:24 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  61202244Ki
  hugepages-2Mi:      0
  memory:             4025488Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  61202244Ki
  hugepages-2Mi:      0
  memory:             4025488Ki
  pods:               110
System Info:
  Machine ID:                 65e3cb62f6f347daa4cd1f1effec1e86
  System UUID:                65e3cb62f6f347daa4cd1f1effec1e86
  Boot ID:                    ced46ab0-5215-4916-a38c-4e8a00025271
  Kernel Version:             6.4.16-linuxkit
  OS Image:                   Ubuntu 22.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.4
  Kube-Proxy Version:         v1.27.4
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                    ------------  ----------  ---------------  -------------  ---
  default                     nginx-77b4fdf86c-vnjmc                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         97d
  default                     todoitem-deployment-849864ff87-2tqv5    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6m42s
  default                     todoitem-deployment-849864ff87-j22mf    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6m42s
  default                     todoitem-deployment-849864ff87-mj5fm    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6m42s
  kube-system                 coredns-5d78c9869d-46l2d                100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     110d
  kube-system                 etcd-minikube                           100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         110d
  kube-system                 kube-apiserver-minikube                 250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         110d
  kube-system                 kube-controller-manager-minikube        200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         110d
  kube-system                 kube-proxy-wbb49                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         110d
  kube-system                 kube-scheduler-minikube                 100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         110d
  kube-system                 storage-provisioner                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         110d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (4%!)(MISSING)  170Mi (4%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 7m17s                  kube-proxy       
  Normal  NodeHasSufficientMemory  97d (x29 over 97d)     kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    97d (x29 over 97d)     kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     97d (x29 over 97d)     kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeReady                97d (x2 over 97d)      kubelet          Node minikube status is now: NodeReady
  Normal  Starting                 96d                    kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  96d                    kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientPID     96d (x7 over 96d)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeHasNoDiskPressure    96d (x8 over 96d)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientMemory  96d (x8 over 96d)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  RegisteredNode           96d                    node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 7m32s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  7m32s (x8 over 7m32s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    7m32s (x8 over 7m32s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     7m32s (x7 over 7m32s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  7m32s                  kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           7m6s                   node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Feb25 04:12] tsc: Unable to calibrate against PIT
[  +0.122491] PCI: Fatal: No config space access function found
[  +0.169049] pci 0000:00:1f.0: BAR 13: [io  size 0x0080] has bogus alignment
[  +0.030140] virtio-pci 0000:00:01.0: can't derive routing for PCI INT A
[  +0.000003] virtio-pci 0000:00:01.0: PCI INT A: no GSI
[  +0.003936] virtio-pci 0000:00:05.0: can't derive routing for PCI INT A
[  +0.000002] virtio-pci 0000:00:05.0: PCI INT A: no GSI
[  +0.003802] virtio-pci 0000:00:06.0: can't derive routing for PCI INT A
[  +0.000002] virtio-pci 0000:00:06.0: PCI INT A: no GSI
[  +0.000874] virtio-pci 0000:00:07.0: can't derive routing for PCI INT A
[  +0.000002] virtio-pci 0000:00:07.0: PCI INT A: no GSI
[  +0.006674] virtio-pci 0000:00:08.0: can't derive routing for PCI INT A
[  +0.000003] virtio-pci 0000:00:08.0: PCI INT A: no GSI
[  +0.006506] virtio-pci 0000:00:09.0: can't derive routing for PCI INT A
[  +0.000003] virtio-pci 0000:00:09.0: PCI INT A: no GSI
[  +0.005900] virtio-pci 0000:00:0a.0: can't derive routing for PCI INT A
[  +0.000002] virtio-pci 0000:00:0a.0: PCI INT A: no GSI
[  +0.007063] virtio-pci 0000:00:0b.0: can't derive routing for PCI INT A
[  +0.000002] virtio-pci 0000:00:0b.0: PCI INT A: no GSI
[  +0.007219] virtio-pci 0000:00:0c.0: can't derive routing for PCI INT A
[  +0.000002] virtio-pci 0000:00:0c.0: PCI INT A: no GSI
[  +0.003652] virtio-pci 0000:00:0d.0: can't derive routing for PCI INT A
[  +0.000003] virtio-pci 0000:00:0d.0: PCI INT A: no GSI
[  +0.003108] virtio-pci 0000:00:0e.0: can't derive routing for PCI INT A
[  +0.000002] virtio-pci 0000:00:0e.0: PCI INT A: no GSI
[  +0.022832] Hangcheck: starting hangcheck timer 0.9.1 (tick is 180 seconds, margin is 60 seconds).
[  +0.061912] lpc_ich 0000:00:1f.0: No MFD cells added
[  +0.022732] fail to initialize ptp_kvm
[  +0.000004] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +4.363523] 3[243]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +1.183317] FAT-fs (loop0): utf8 is not a recommended IO charset for FAT filesystems, filesystem will be case sensitive!
[  +0.001166] FAT-fs (loop0): utf8 is not a recommended IO charset for FAT filesystems, filesystem will be case sensitive!
[  +0.071345] grpcfuse: loading out-of-tree module taints kernel.
[Feb25 04:14] hrtimer: interrupt took 5520910 ns

* 
* ==> etcd [2bbabf72d575] <==
* {"level":"info","ts":"2024-02-25T04:16:57.808Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-02-25T04:16:57.811Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2024-02-25T04:16:57.811Z","caller":"embed/etcd.go:124","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-02-25T04:16:57.814Z","caller":"embed/etcd.go:484","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-02-25T04:16:57.870Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-02-25T04:16:57.872Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.7","git-sha":"215b53cf3","go-version":"go1.17.13","go-os":"linux","go-arch":"amd64","max-cpu-set":4,"max-cpu-available":4,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-02-25T04:16:57.907Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"34.254209ms"}
{"level":"info","ts":"2024-02-25T04:16:58.784Z","caller":"etcdserver/server.go:509","msg":"recovered v2 store from snapshot","snapshot-index":10001,"snapshot-size":"7.5 kB"}
{"level":"info","ts":"2024-02-25T04:16:58.784Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":2416640,"backend-size":"2.4 MB","backend-size-in-use-bytes":1175552,"backend-size-in-use":"1.2 MB"}
{"level":"info","ts":"2024-02-25T04:16:59.278Z","caller":"etcdserver/raft.go:529","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":18423}
{"level":"info","ts":"2024-02-25T04:16:59.297Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-02-25T04:16:59.298Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 5"}
{"level":"info","ts":"2024-02-25T04:16:59.298Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 5, commit: 18423, applied: 10001, lastindex: 18423, lastterm: 5]"}
{"level":"info","ts":"2024-02-25T04:16:59.299Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-02-25T04:16:59.299Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-02-25T04:16:59.300Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2024-02-25T04:16:59.305Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-02-25T04:16:59.308Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":14352}
{"level":"info","ts":"2024-02-25T04:16:59.311Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":14742}
{"level":"info","ts":"2024-02-25T04:16:59.316Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-02-25T04:16:59.365Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-02-25T04:16:59.367Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-02-25T04:16:59.368Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.7","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2024-02-25T04:16:59.369Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-02-25T04:16:59.372Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-25T04:16:59.372Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-25T04:16:59.380Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-25T04:16:59.381Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-02-25T04:16:59.382Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-25T04:16:59.384Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-25T04:16:59.384Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-02-25T04:16:59.383Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-02-25T04:16:59.901Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 5"}
{"level":"info","ts":"2024-02-25T04:16:59.901Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 5"}
{"level":"info","ts":"2024-02-25T04:16:59.901Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2024-02-25T04:16:59.901Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 6"}
{"level":"info","ts":"2024-02-25T04:16:59.901Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 6"}
{"level":"info","ts":"2024-02-25T04:16:59.901Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 6"}
{"level":"info","ts":"2024-02-25T04:16:59.901Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 6"}
{"level":"info","ts":"2024-02-25T04:16:59.909Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-02-25T04:16:59.909Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-02-25T04:16:59.910Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-02-25T04:16:59.910Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-02-25T04:16:59.910Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-02-25T04:16:59.914Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-02-25T04:16:59.916Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"warn","ts":"2024-02-25T04:17:05.569Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"281.592512ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128027414182733176 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:14738 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:599 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >>","response":"size:16"}
{"level":"info","ts":"2024-02-25T04:17:05.570Z","caller":"traceutil/trace.go:171","msg":"trace[1925513699] linearizableReadLoop","detail":"{readStateIndex:18427; appliedIndex:18426; }","duration":"283.603886ms","start":"2024-02-25T04:17:05.286Z","end":"2024-02-25T04:17:05.570Z","steps":["trace[1925513699] 'read index received'  (duration: 118.847µs)","trace[1925513699] 'applied index is now lower than readState.Index'  (duration: 283.480187ms)"],"step_count":2}
{"level":"info","ts":"2024-02-25T04:17:05.570Z","caller":"traceutil/trace.go:171","msg":"trace[2062523746] transaction","detail":"{read_only:false; response_revision:14743; number_of_response:1; }","duration":"291.419251ms","start":"2024-02-25T04:17:05.279Z","end":"2024-02-25T04:17:05.570Z","steps":["trace[2062523746] 'compare'  (duration: 183.936733ms)","trace[2062523746] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii; req_size:669; } (duration: 97.408983ms)"],"step_count":2}
{"level":"warn","ts":"2024-02-25T04:17:05.571Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"284.331489ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/priorityclasses/system-cluster-critical\" ","response":"range_response_count:1 size:477"}
{"level":"info","ts":"2024-02-25T04:17:05.571Z","caller":"traceutil/trace.go:171","msg":"trace[1018730735] range","detail":"{range_begin:/registry/priorityclasses/system-cluster-critical; range_end:; response_count:1; response_revision:14744; }","duration":"284.438656ms","start":"2024-02-25T04:17:05.286Z","end":"2024-02-25T04:17:05.571Z","steps":["trace[1018730735] 'agreement among raft nodes before linearized reading'  (duration: 284.294482ms)"],"step_count":1}
{"level":"warn","ts":"2024-02-25T04:17:05.571Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"184.867026ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" ","response":"range_response_count:1 size:687"}
{"level":"info","ts":"2024-02-25T04:17:05.571Z","caller":"traceutil/trace.go:171","msg":"trace[2089465059] range","detail":"{range_begin:/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii; range_end:; response_count:1; response_revision:14744; }","duration":"184.918263ms","start":"2024-02-25T04:17:05.386Z","end":"2024-02-25T04:17:05.571Z","steps":["trace[2089465059] 'agreement among raft nodes before linearized reading'  (duration: 184.827763ms)"],"step_count":1}
{"level":"info","ts":"2024-02-25T04:17:05.572Z","caller":"traceutil/trace.go:171","msg":"trace[640558147] transaction","detail":"{read_only:false; number_of_response:0; response_revision:14743; }","duration":"202.072336ms","start":"2024-02-25T04:17:05.370Z","end":"2024-02-25T04:17:05.572Z","steps":["trace[640558147] 'process raft request'  (duration: 198.923217ms)"],"step_count":1}
{"level":"warn","ts":"2024-02-25T04:17:05.579Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"184.34503ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterrolebindings/\" range_end:\"/registry/clusterrolebindings0\" ","response":"range_response_count:51 size:36246"}
{"level":"info","ts":"2024-02-25T04:17:05.579Z","caller":"traceutil/trace.go:171","msg":"trace[2142245618] range","detail":"{range_begin:/registry/clusterrolebindings/; range_end:/registry/clusterrolebindings0; response_count:51; response_revision:14744; }","duration":"184.89727ms","start":"2024-02-25T04:17:05.394Z","end":"2024-02-25T04:17:05.579Z","steps":["trace[2142245618] 'agreement among raft nodes before linearized reading'  (duration: 178.211765ms)"],"step_count":1}
{"level":"info","ts":"2024-02-25T04:17:37.994Z","caller":"traceutil/trace.go:171","msg":"trace[906353425] transaction","detail":"{read_only:false; response_revision:14853; number_of_response:1; }","duration":"110.684459ms","start":"2024-02-25T04:17:37.883Z","end":"2024-02-25T04:17:37.994Z","steps":["trace[906353425] 'process raft request'  (duration: 100.557531ms)"],"step_count":1}
{"level":"info","ts":"2024-02-25T04:18:04.000Z","caller":"traceutil/trace.go:171","msg":"trace[642840849] transaction","detail":"{read_only:false; response_revision:14937; number_of_response:1; }","duration":"114.408298ms","start":"2024-02-25T04:18:03.886Z","end":"2024-02-25T04:18:04.000Z","steps":["trace[642840849] 'process raft request'  (duration: 114.151693ms)"],"step_count":1}
{"level":"info","ts":"2024-02-25T04:18:04.000Z","caller":"traceutil/trace.go:171","msg":"trace[1145132658] transaction","detail":"{read_only:false; response_revision:14938; number_of_response:1; }","duration":"103.948625ms","start":"2024-02-25T04:18:03.896Z","end":"2024-02-25T04:18:04.000Z","steps":["trace[1145132658] 'process raft request'  (duration: 103.886565ms)"],"step_count":1}
{"level":"info","ts":"2024-02-25T04:22:25.835Z","caller":"traceutil/trace.go:171","msg":"trace[1693825417] transaction","detail":"{read_only:false; response_revision:15228; number_of_response:1; }","duration":"101.368355ms","start":"2024-02-25T04:22:25.734Z","end":"2024-02-25T04:22:25.835Z","steps":["trace[1693825417] 'process raft request'  (duration: 101.187222ms)"],"step_count":1}

* 
* ==> etcd [41595d4aac9e] <==
* {"level":"info","ts":"2023-11-20T04:38:49.289Z","caller":"traceutil/trace.go:171","msg":"trace[785471906] range","detail":"{range_begin:/registry/replicasets/kube-system/coredns-5d78c9869d; range_end:; response_count:1; response_revision:14725; }","duration":"320.993529ms","start":"2023-11-20T04:38:48.968Z","end":"2023-11-20T04:38:49.289Z","steps":["trace[785471906] 'agreement among raft nodes before linearized reading'  (duration: 221.479561ms)","trace[785471906] 'range keys from in-memory index tree'  (duration: 99.355844ms)"],"step_count":2}
{"level":"warn","ts":"2023-11-20T04:38:49.289Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-20T04:38:48.968Z","time spent":"321.064989ms","remote":"127.0.0.1:47938","response type":"/etcdserverpb.KV/Range","request count":0,"request size":54,"response count":1,"response size":3821,"request content":"key:\"/registry/replicasets/kube-system/coredns-5d78c9869d\" "}
{"level":"warn","ts":"2023-11-20T04:38:49.289Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"286.680534ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/default/nginx-77b4fdf86c\" ","response":"range_response_count:1 size:1704"}
{"level":"info","ts":"2023-11-20T04:38:49.290Z","caller":"traceutil/trace.go:171","msg":"trace[936241857] range","detail":"{range_begin:/registry/replicasets/default/nginx-77b4fdf86c; range_end:; response_count:1; response_revision:14725; }","duration":"286.891833ms","start":"2023-11-20T04:38:49.003Z","end":"2023-11-20T04:38:49.290Z","steps":["trace[936241857] 'agreement among raft nodes before linearized reading'  (duration: 186.505946ms)","trace[936241857] 'range keys from in-memory index tree'  (duration: 100.110795ms)"],"step_count":2}
{"level":"warn","ts":"2023-11-20T04:38:49.290Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"313.51823ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/daemonsets/kube-system/kube-proxy\" ","response":"range_response_count:1 size:2895"}
{"level":"info","ts":"2023-11-20T04:38:49.290Z","caller":"traceutil/trace.go:171","msg":"trace[331601810] range","detail":"{range_begin:/registry/daemonsets/kube-system/kube-proxy; range_end:; response_count:1; response_revision:14725; }","duration":"313.565427ms","start":"2023-11-20T04:38:48.976Z","end":"2023-11-20T04:38:49.290Z","steps":["trace[331601810] 'agreement among raft nodes before linearized reading'  (duration: 212.799281ms)","trace[331601810] 'range keys from in-memory index tree'  (duration: 100.667376ms)"],"step_count":2}
{"level":"warn","ts":"2023-11-20T04:38:49.777Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-20T04:38:48.976Z","time spent":"800.224464ms","remote":"127.0.0.1:47922","response type":"/etcdserverpb.KV/Range","request count":0,"request size":45,"response count":1,"response size":2919,"request content":"key:\"/registry/daemonsets/kube-system/kube-proxy\" "}
{"level":"info","ts":"2023-11-20T04:38:49.842Z","caller":"traceutil/trace.go:171","msg":"trace[1150468618] transaction","detail":"{read_only:false; response_revision:14726; number_of_response:1; }","duration":"638.274258ms","start":"2023-11-20T04:38:49.204Z","end":"2023-11-20T04:38:49.842Z","steps":["trace[1150468618] 'process raft request'  (duration: 591.758767ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-20T04:38:49.857Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-20T04:38:49.204Z","time spent":"652.412519ms","remote":"127.0.0.1:47902","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1954,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/deployments/default/nginx\" mod_revision:13427 > success:<request_put:<key:\"/registry/deployments/default/nginx\" value_size:1911 >> failure:<request_range:<key:\"/registry/deployments/default/nginx\" > >"}
{"level":"info","ts":"2023-11-20T04:38:49.867Z","caller":"traceutil/trace.go:171","msg":"trace[2028811967] linearizableReadLoop","detail":"{readStateIndex:18403; appliedIndex:18403; }","duration":"303.408224ms","start":"2023-11-20T04:38:49.564Z","end":"2023-11-20T04:38:49.867Z","steps":["trace[2028811967] 'read index received'  (duration: 303.393327ms)","trace[2028811967] 'applied index is now lower than readState.Index'  (duration: 12.727µs)"],"step_count":2}
{"level":"warn","ts":"2023-11-20T04:38:49.868Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"303.656054ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/daemon-set-controller\" ","response":"range_response_count:1 size:207"}
{"level":"info","ts":"2023-11-20T04:38:49.868Z","caller":"traceutil/trace.go:171","msg":"trace[409491581] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/daemon-set-controller; range_end:; response_count:1; response_revision:14726; }","duration":"303.721186ms","start":"2023-11-20T04:38:49.564Z","end":"2023-11-20T04:38:49.868Z","steps":["trace[409491581] 'agreement among raft nodes before linearized reading'  (duration: 303.560341ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-20T04:38:49.868Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-20T04:38:49.564Z","time spent":"303.793924ms","remote":"127.0.0.1:48426","response type":"/etcdserverpb.KV/Range","request count":0,"request size":61,"response count":1,"response size":231,"request content":"key:\"/registry/serviceaccounts/kube-system/daemon-set-controller\" "}
{"level":"warn","ts":"2023-11-20T04:38:49.956Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"303.846472ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/kube-system/coredns\" ","response":"range_response_count:1 size:4121"}
{"level":"info","ts":"2023-11-20T04:38:49.956Z","caller":"traceutil/trace.go:171","msg":"trace[991373548] range","detail":"{range_begin:/registry/deployments/kube-system/coredns; range_end:; response_count:1; response_revision:14727; }","duration":"303.911823ms","start":"2023-11-20T04:38:49.652Z","end":"2023-11-20T04:38:49.956Z","steps":["trace[991373548] 'agreement among raft nodes before linearized reading'  (duration: 303.777938ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-20T04:38:49.956Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-20T04:38:49.652Z","time spent":"303.963769ms","remote":"127.0.0.1:47902","response type":"/etcdserverpb.KV/Range","request count":0,"request size":43,"response count":1,"response size":4145,"request content":"key:\"/registry/deployments/kube-system/coredns\" "}
{"level":"info","ts":"2023-11-20T04:38:49.956Z","caller":"traceutil/trace.go:171","msg":"trace[857225669] transaction","detail":"{read_only:false; response_revision:14727; number_of_response:1; }","duration":"304.299474ms","start":"2023-11-20T04:38:49.652Z","end":"2023-11-20T04:38:49.956Z","steps":["trace[857225669] 'process raft request'  (duration: 294.682807ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-20T04:38:49.957Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-20T04:38:49.652Z","time spent":"304.360208ms","remote":"127.0.0.1:47902","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":4106,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/deployments/kube-system/coredns\" mod_revision:14699 > success:<request_put:<key:\"/registry/deployments/kube-system/coredns\" value_size:4057 >> failure:<request_range:<key:\"/registry/deployments/kube-system/coredns\" > >"}
{"level":"warn","ts":"2023-11-20T04:38:49.957Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"184.910358ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/default/nginx\" ","response":"range_response_count:1 size:1969"}
{"level":"info","ts":"2023-11-20T04:38:49.957Z","caller":"traceutil/trace.go:171","msg":"trace[2070493388] range","detail":"{range_begin:/registry/deployments/default/nginx; range_end:; response_count:1; response_revision:14727; }","duration":"184.934353ms","start":"2023-11-20T04:38:49.772Z","end":"2023-11-20T04:38:49.957Z","steps":["trace[2070493388] 'agreement among raft nodes before linearized reading'  (duration: 184.880011ms)"],"step_count":1}
{"level":"info","ts":"2023-11-20T04:38:50.465Z","caller":"traceutil/trace.go:171","msg":"trace[2028570238] transaction","detail":"{read_only:false; response_revision:14728; number_of_response:1; }","duration":"114.25437ms","start":"2023-11-20T04:38:50.351Z","end":"2023-11-20T04:38:50.465Z","steps":["trace[2028570238] 'process raft request'  (duration: 113.462002ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-20T04:38:50.557Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"179.995825ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-11-20T04:38:50.557Z","caller":"traceutil/trace.go:171","msg":"trace[1552220630] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:14728; }","duration":"180.156389ms","start":"2023-11-20T04:38:50.377Z","end":"2023-11-20T04:38:50.557Z","steps":["trace[1552220630] 'agreement among raft nodes before linearized reading'  (duration: 87.247401ms)","trace[1552220630] 'filter and sort the key-value pairs'  (duration: 92.603393ms)"],"step_count":2}
{"level":"info","ts":"2023-11-20T04:38:51.084Z","caller":"traceutil/trace.go:171","msg":"trace[1129772368] transaction","detail":"{read_only:false; response_revision:14730; number_of_response:1; }","duration":"130.170107ms","start":"2023-11-20T04:38:50.954Z","end":"2023-11-20T04:38:51.084Z","steps":["trace[1129772368] 'process raft request'  (duration: 118.169847ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-20T04:38:52.052Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"265.348152ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/default\" ","response":"range_response_count:1 size:339"}
{"level":"info","ts":"2023-11-20T04:38:52.052Z","caller":"traceutil/trace.go:171","msg":"trace[2137157994] range","detail":"{range_begin:/registry/namespaces/default; range_end:; response_count:1; response_revision:14730; }","duration":"265.415065ms","start":"2023-11-20T04:38:51.787Z","end":"2023-11-20T04:38:52.052Z","steps":["trace[2137157994] 'range keys from in-memory index tree'  (duration: 263.010865ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-20T04:38:52.065Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"272.725031ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-11-20T04:38:52.066Z","caller":"traceutil/trace.go:171","msg":"trace[458956477] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:14730; }","duration":"273.079478ms","start":"2023-11-20T04:38:51.792Z","end":"2023-11-20T04:38:52.066Z","steps":["trace[458956477] 'range keys from in-memory index tree'  (duration: 272.364527ms)"],"step_count":1}
{"level":"info","ts":"2023-11-20T04:38:52.150Z","caller":"traceutil/trace.go:171","msg":"trace[294028291] transaction","detail":"{read_only:false; response_revision:14731; number_of_response:1; }","duration":"175.817266ms","start":"2023-11-20T04:38:51.974Z","end":"2023-11-20T04:38:52.150Z","steps":["trace[294028291] 'process raft request'  (duration: 175.545801ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-20T04:38:56.165Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"112.728576ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128025269019008570 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/kube-system/storage-provisioner.17993aefcf06ab69\" mod_revision:0 > success:<request_put:<key:\"/registry/events/kube-system/storage-provisioner.17993aefcf06ab69\" value_size:584 lease:8128025269019008123 >> failure:<>>","response":"size:16"}
{"level":"info","ts":"2023-11-20T04:38:56.166Z","caller":"traceutil/trace.go:171","msg":"trace[1977849572] transaction","detail":"{read_only:false; response_revision:14735; number_of_response:1; }","duration":"198.084233ms","start":"2023-11-20T04:38:55.967Z","end":"2023-11-20T04:38:56.166Z","steps":["trace[1977849572] 'process raft request'  (duration: 84.944852ms)","trace[1977849572] 'compare'  (duration: 112.607082ms)"],"step_count":2}
{"level":"warn","ts":"2023-11-20T04:38:58.957Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"192.035913ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/configmaps/kube-system/coredns\" ","response":"range_response_count:1 size:766"}
{"level":"info","ts":"2023-11-20T04:38:58.957Z","caller":"traceutil/trace.go:171","msg":"trace[736313406] range","detail":"{range_begin:/registry/configmaps/kube-system/coredns; range_end:; response_count:1; response_revision:14735; }","duration":"192.10281ms","start":"2023-11-20T04:38:58.765Z","end":"2023-11-20T04:38:58.957Z","steps":["trace[736313406] 'range keys from bolt db'  (duration: 191.696062ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-20T04:39:02.777Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"307.011899ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128025269019008581 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:14728 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:599 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >>","response":"size:16"}
{"level":"info","ts":"2023-11-20T04:39:02.777Z","caller":"traceutil/trace.go:171","msg":"trace[787390284] linearizableReadLoop","detail":"{readStateIndex:18417; appliedIndex:18416; }","duration":"306.689725ms","start":"2023-11-20T04:39:02.470Z","end":"2023-11-20T04:39:02.777Z","steps":["trace[787390284] 'read index received'  (duration: 115.275µs)","trace[787390284] 'applied index is now lower than readState.Index'  (duration: 306.572595ms)"],"step_count":2}
{"level":"info","ts":"2023-11-20T04:39:02.777Z","caller":"traceutil/trace.go:171","msg":"trace[1096136356] transaction","detail":"{read_only:false; response_revision:14738; number_of_response:1; }","duration":"310.113083ms","start":"2023-11-20T04:39:02.467Z","end":"2023-11-20T04:39:02.777Z","steps":["trace[1096136356] 'compare'  (duration: 306.884629ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-20T04:39:02.777Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-20T04:39:02.467Z","time spent":"310.205174ms","remote":"127.0.0.1:48508","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":672,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:14728 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:599 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2023-11-20T04:39:02.779Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"308.640062ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2023-11-20T04:39:02.779Z","caller":"traceutil/trace.go:171","msg":"trace[1654862763] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:14738; }","duration":"308.724594ms","start":"2023-11-20T04:39:02.470Z","end":"2023-11-20T04:39:02.779Z","steps":["trace[1654862763] 'agreement among raft nodes before linearized reading'  (duration: 308.525514ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-20T04:39:02.779Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-20T04:39:02.470Z","time spent":"308.79448ms","remote":"127.0.0.1:54390","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2023-11-20T04:39:03.788Z","caller":"traceutil/trace.go:171","msg":"trace[779196666] transaction","detail":"{read_only:false; response_revision:14739; number_of_response:1; }","duration":"603.145036ms","start":"2023-11-20T04:39:03.184Z","end":"2023-11-20T04:39:03.788Z","steps":["trace[779196666] 'process raft request'  (duration: 603.002319ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-20T04:39:03.788Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-20T04:39:03.184Z","time spent":"603.283975ms","remote":"127.0.0.1:48412","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":4516,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/pods/kube-system/kube-proxy-wbb49\" mod_revision:14696 > success:<request_put:<key:\"/registry/pods/kube-system/kube-proxy-wbb49\" value_size:4465 >> failure:<request_range:<key:\"/registry/pods/kube-system/kube-proxy-wbb49\" > >"}
{"level":"warn","ts":"2023-11-20T04:39:04.689Z","caller":"etcdserver/v3_server.go:840","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128025269019008584,"retry-timeout":"500ms"}
{"level":"info","ts":"2023-11-20T04:39:04.875Z","caller":"traceutil/trace.go:171","msg":"trace[627000979] linearizableReadLoop","detail":"{readStateIndex:18418; appliedIndex:18418; }","duration":"1.302135961s","start":"2023-11-20T04:39:03.573Z","end":"2023-11-20T04:39:04.875Z","steps":["trace[627000979] 'read index received'  (duration: 1.302120871s)","trace[627000979] 'applied index is now lower than readState.Index'  (duration: 10.313µs)"],"step_count":2}
{"level":"warn","ts":"2023-11-20T04:39:05.271Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.697297171s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2023-11-20T04:39:05.271Z","caller":"traceutil/trace.go:171","msg":"trace[1385294261] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:14739; }","duration":"1.697396897s","start":"2023-11-20T04:39:03.574Z","end":"2023-11-20T04:39:05.271Z","steps":["trace[1385294261] 'agreement among raft nodes before linearized reading'  (duration: 1.300891592s)","trace[1385294261] 'range keys from in-memory index tree'  (duration: 396.319385ms)"],"step_count":2}
{"level":"warn","ts":"2023-11-20T04:39:05.271Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-20T04:39:03.574Z","time spent":"1.697487153s","remote":"127.0.0.1:48294","response type":"/etcdserverpb.KV/Range","request count":0,"request size":37,"response count":1,"response size":155,"request content":"key:\"/registry/masterleases/192.168.49.2\" "}
{"level":"warn","ts":"2023-11-20T04:39:05.272Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.699177426s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/\" range_end:\"/registry/events0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2023-11-20T04:39:05.272Z","caller":"traceutil/trace.go:171","msg":"trace[231543870] range","detail":"{range_begin:/registry/events/; range_end:/registry/events0; response_count:0; response_revision:14739; }","duration":"1.699235354s","start":"2023-11-20T04:39:03.573Z","end":"2023-11-20T04:39:05.272Z","steps":["trace[231543870] 'agreement among raft nodes before linearized reading'  (duration: 1.302367016s)","trace[231543870] 'count revisions from in-memory index tree'  (duration: 396.778122ms)"],"step_count":2}
{"level":"warn","ts":"2023-11-20T04:39:05.272Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-20T04:39:03.573Z","time spent":"1.699305033s","remote":"127.0.0.1:48312","response type":"/etcdserverpb.KV/Range","request count":0,"request size":40,"response count":60,"response size":31,"request content":"key:\"/registry/events/\" range_end:\"/registry/events0\" count_only:true "}
{"level":"info","ts":"2023-11-20T04:39:06.884Z","caller":"traceutil/trace.go:171","msg":"trace[743630931] transaction","detail":"{read_only:false; response_revision:14741; number_of_response:1; }","duration":"377.755874ms","start":"2023-11-20T04:39:06.506Z","end":"2023-11-20T04:39:06.884Z","steps":["trace[743630931] 'process raft request'  (duration: 377.595766ms)"],"step_count":1}
{"level":"warn","ts":"2023-11-20T04:39:06.884Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-20T04:39:06.506Z","time spent":"377.962689ms","remote":"127.0.0.1:47922","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":2880,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/daemonsets/kube-system/kube-proxy\" mod_revision:14729 > success:<request_put:<key:\"/registry/daemonsets/kube-system/kube-proxy\" value_size:2829 >> failure:<request_range:<key:\"/registry/daemonsets/kube-system/kube-proxy\" > >"}
{"level":"warn","ts":"2023-11-20T04:39:07.018Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.244087ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:420"}
{"level":"info","ts":"2023-11-20T04:39:07.080Z","caller":"traceutil/trace.go:171","msg":"trace[1017101220] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:14741; }","duration":"162.470656ms","start":"2023-11-20T04:39:06.918Z","end":"2023-11-20T04:39:07.080Z","steps":["trace[1017101220] 'range keys from in-memory index tree'  (duration: 99.342355ms)"],"step_count":1}
{"level":"info","ts":"2023-11-20T04:39:08.685Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2023-11-20T04:39:08.685Z","caller":"embed/etcd.go:373","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-11-20T04:39:10.991Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-11-20T04:39:11.009Z","caller":"embed/etcd.go:568","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-11-20T04:39:11.083Z","caller":"embed/etcd.go:573","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-11-20T04:39:11.084Z","caller":"embed/etcd.go:375","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> kernel <==
*  04:24:27 up 12 min,  0 users,  load average: 2.16, 1.52, 0.97
Linux minikube 6.4.16-linuxkit #1 SMP PREEMPT_DYNAMIC Tue Oct 10 20:42:40 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.2 LTS"

* 
* ==> kube-apiserver [edd444775bf6] <==
* I0225 04:17:04.099750       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0225 04:17:04.099907       1 apf_controller.go:361] Starting API Priority and Fairness config controller
I0225 04:17:04.100307       1 controller.go:121] Starting legacy_token_tracking_controller
I0225 04:17:04.100354       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0225 04:17:04.100552       1 aggregator.go:150] waiting for initial CRD sync...
I0225 04:17:04.092739       1 handler_discovery.go:392] Starting ResourceDiscoveryManager
I0225 04:17:04.099761       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0225 04:17:04.093408       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0225 04:17:04.099782       1 controller.go:83] Starting OpenAPI AggregationController
I0225 04:17:04.100672       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0225 04:17:04.100695       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0225 04:17:04.111054       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0225 04:17:04.177950       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0225 04:17:04.178086       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0225 04:17:04.369277       1 controller.go:85] Starting OpenAPI controller
I0225 04:17:04.369720       1 controller.go:85] Starting OpenAPI V3 controller
I0225 04:17:04.369874       1 naming_controller.go:291] Starting NamingConditionController
I0225 04:17:04.369945       1 establishing_controller.go:76] Starting EstablishingController
I0225 04:17:04.371576       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0225 04:17:04.371901       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0225 04:17:04.372107       1 crd_finalizer.go:266] Starting CRDFinalizer
I0225 04:17:04.696962       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0225 04:17:04.919955       1 trace.go:219] Trace[1541440384]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:079602ae-5a22-4413-947d-013c010988e6,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:GET (25-Feb-2024 04:17:04.399) (total time: 520ms):
Trace[1541440384]: ---"About to write a response" 327ms (04:17:04.726)
Trace[1541440384]: ---"Writing http response done" 193ms (04:17:04.919)
Trace[1541440384]: [520.766451ms] [520.766451ms] END
I0225 04:17:05.078807       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0225 04:17:05.079320       1 shared_informer.go:318] Caches are synced for configmaps
I0225 04:17:05.087649       1 aggregator.go:152] initial CRD sync complete...
I0225 04:17:05.087674       1 autoregister_controller.go:141] Starting autoregister controller
I0225 04:17:05.087690       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0225 04:17:05.087707       1 cache.go:39] Caches are synced for autoregister controller
I0225 04:17:05.100215       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0225 04:17:05.119489       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0225 04:17:05.121840       1 apf_controller.go:366] Running API Priority and Fairness config worker
I0225 04:17:05.121862       1 apf_controller.go:369] Running API Priority and Fairness periodic rebalancing process
I0225 04:17:05.123013       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0225 04:17:05.135418       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0225 04:17:05.155030       1 shared_informer.go:318] Caches are synced for node_authorizer
I0225 04:17:05.580142       1 trace.go:219] Trace[455030359]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:e28ce6d0-83e9-4413-9b21-88ab7b31905a,client:192.168.49.2,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:POST (25-Feb-2024 04:17:04.179) (total time: 1400ms):
Trace[455030359]: [1.400991957s] [1.400991957s] END
I0225 04:17:05.591173       1 trace.go:219] Trace[1286787905]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:7f8bc324-3744-448b-8371-846a641517ed,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:POST (25-Feb-2024 04:17:04.177) (total time: 1413ms):
Trace[1286787905]: [1.413982811s] [1.413982811s] END
I0225 04:17:05.595249       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0225 04:17:09.992377       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0225 04:17:10.033022       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0225 04:17:10.259752       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0225 04:17:10.388909       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0225 04:17:10.410884       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
E0225 04:17:15.693479       1 controller.go:193] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 2bc848df-9a64-4700-909f-6c8eeb546e37, UID in object meta: "
I0225 04:17:29.008555       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0225 04:17:29.011471       1 controller.go:624] quota admission added evaluator for: endpoints
I0225 04:17:36.597395       1 trace.go:219] Trace[350491836]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:2bc43c37-f4bd-4990-af17-41bd63d5278a,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PUT (25-Feb-2024 04:17:36.024) (total time: 570ms):
Trace[350491836]: ["GuaranteedUpdate etcd3" audit-id:2bc43c37-f4bd-4990-af17-41bd63d5278a,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 569ms (04:17:36.026)
Trace[350491836]:  ---"About to Encode" 171ms (04:17:36.198)
Trace[350491836]:  ---"Encode succeeded" len:599 65ms (04:17:36.263)
Trace[350491836]:  ---"Txn call completed" 326ms (04:17:36.590)]
Trace[350491836]: [570.739851ms] [570.739851ms] END
I0225 04:17:44.171017       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I0225 04:18:03.804493       1 alloc.go:330] "allocated clusterIPs" service="default/todoitem-service" clusterIPs=map[IPv4:10.104.8.247]

* 
* ==> kube-apiserver [ef15122f1653] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1120 04:39:12.008335       1 logging.go:59] [core] [Channel #76 SubChannel #77] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1120 04:39:12.017843       1 logging.go:59] [core] [Channel #169 SubChannel #170] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1120 04:39:12.754240       1 logging.go:59] [core] [Channel #118 SubChannel #119] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1120 04:39:12.754369       1 logging.go:59] [core] [Channel #67 SubChannel #68] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1120 04:39:12.955666       1 logging.go:59] [core] [Channel #151 SubChannel #152] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1120 04:39:12.973821       1 logging.go:59] [core] [Channel #97 SubChannel #98] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: operation was canceled"
W1120 04:39:13.106687       1 logging.go:59] [core] [Channel #178 SubChannel #179] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-controller-manager [58180cecd3e7] <==
* I0225 04:17:20.417990       1 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/minikube/certs/ca.crt::/var/lib/minikube/certs/ca.key"
I0225 04:17:20.418007       1 dynamic_serving_content.go:132] "Starting controller" name="csr-controller::/var/lib/minikube/certs/ca.crt::/var/lib/minikube/certs/ca.key"
I0225 04:17:20.476382       1 controllermanager.go:638] "Started controller" controller="csrapproving"
I0225 04:17:20.476870       1 certificate_controller.go:112] Starting certificate controller "csrapproving"
I0225 04:17:20.476929       1 shared_informer.go:311] Waiting for caches to sync for certificate-csrapproving
I0225 04:17:20.526768       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0225 04:17:20.528782       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I0225 04:17:20.530160       1 shared_informer.go:318] Caches are synced for TTL
I0225 04:17:20.581428       1 shared_informer.go:318] Caches are synced for node
I0225 04:17:20.581640       1 range_allocator.go:174] "Sending events to api server"
I0225 04:17:20.581719       1 range_allocator.go:178] "Starting range CIDR allocator"
I0225 04:17:20.581730       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0225 04:17:20.581747       1 shared_informer.go:318] Caches are synced for cidrallocator
I0225 04:17:20.583429       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0225 04:17:20.685069       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0225 04:17:20.685363       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0225 04:17:20.685585       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0225 04:17:20.695473       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0225 04:17:20.696987       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0225 04:17:20.702646       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0225 04:17:20.704256       1 shared_informer.go:318] Caches are synced for daemon sets
I0225 04:17:20.704352       1 shared_informer.go:318] Caches are synced for HPA
I0225 04:17:20.704376       1 shared_informer.go:318] Caches are synced for crt configmap
I0225 04:17:20.705960       1 shared_informer.go:318] Caches are synced for job
I0225 04:17:20.708006       1 shared_informer.go:318] Caches are synced for deployment
I0225 04:17:20.707546       1 shared_informer.go:318] Caches are synced for GC
I0225 04:17:20.768460       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0225 04:17:20.769021       1 shared_informer.go:318] Caches are synced for PV protection
I0225 04:17:20.770049       1 shared_informer.go:318] Caches are synced for resource quota
I0225 04:17:20.770107       1 shared_informer.go:318] Caches are synced for attach detach
I0225 04:17:20.772336       1 shared_informer.go:318] Caches are synced for ReplicationController
I0225 04:17:20.772951       1 shared_informer.go:318] Caches are synced for cronjob
I0225 04:17:20.774871       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0225 04:17:20.774895       1 shared_informer.go:318] Caches are synced for persistent volume
I0225 04:17:20.774913       1 shared_informer.go:318] Caches are synced for TTL after finished
I0225 04:17:20.774942       1 shared_informer.go:318] Caches are synced for resource quota
I0225 04:17:20.776250       1 shared_informer.go:318] Caches are synced for disruption
I0225 04:17:20.777686       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0225 04:17:20.778348       1 shared_informer.go:318] Caches are synced for ephemeral
I0225 04:17:20.782829       1 shared_informer.go:318] Caches are synced for PVC protection
I0225 04:17:20.787267       1 shared_informer.go:318] Caches are synced for stateful set
I0225 04:17:20.787715       1 shared_informer.go:318] Caches are synced for namespace
I0225 04:17:20.788166       1 shared_informer.go:318] Caches are synced for taint
I0225 04:17:20.788445       1 shared_informer.go:318] Caches are synced for expand
I0225 04:17:20.788669       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0225 04:17:20.793591       1 shared_informer.go:318] Caches are synced for endpoint
I0225 04:17:20.793845       1 taint_manager.go:211] "Sending events to api server"
I0225 04:17:20.789824       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I0225 04:17:20.794082       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0225 04:17:20.794141       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I0225 04:17:20.793553       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0225 04:17:20.794973       1 shared_informer.go:318] Caches are synced for service account
I0225 04:17:20.796367       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0225 04:17:21.103101       1 shared_informer.go:318] Caches are synced for garbage collector
I0225 04:17:21.192469       1 shared_informer.go:318] Caches are synced for garbage collector
I0225 04:17:21.192819       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0225 04:17:44.188841       1 event.go:307] "Event occurred" object="default/todoitem-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set todoitem-deployment-849864ff87 to 3"
I0225 04:17:44.281006       1 event.go:307] "Event occurred" object="default/todoitem-deployment-849864ff87" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: todoitem-deployment-849864ff87-mj5fm"
I0225 04:17:44.365574       1 event.go:307] "Event occurred" object="default/todoitem-deployment-849864ff87" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: todoitem-deployment-849864ff87-2tqv5"
I0225 04:17:44.387211       1 event.go:307] "Event occurred" object="default/todoitem-deployment-849864ff87" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: todoitem-deployment-849864ff87-j22mf"

* 
* ==> kube-controller-manager [a5367f414126] <==
* I1120 04:38:42.087704       1 controllermanager.go:638] "Started controller" controller="horizontalpodautoscaling"
I1120 04:38:42.189327       1 horizontal.go:200] "Starting HPA controller"
I1120 04:38:42.189444       1 shared_informer.go:311] Waiting for caches to sync for HPA
I1120 04:38:42.209334       1 controllermanager.go:638] "Started controller" controller="statefulset"
I1120 04:38:42.348596       1 stateful_set.go:161] "Starting stateful set controller"
I1120 04:38:42.348637       1 shared_informer.go:311] Waiting for caches to sync for stateful set
I1120 04:38:42.656950       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I1120 04:38:43.265031       1 shared_informer.go:318] Caches are synced for service account
I1120 04:38:43.265197       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I1120 04:38:43.385591       1 shared_informer.go:318] Caches are synced for endpoint
I1120 04:38:43.459238       1 shared_informer.go:318] Caches are synced for namespace
I1120 04:38:43.459390       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I1120 04:38:43.464371       1 shared_informer.go:318] Caches are synced for crt configmap
I1120 04:38:43.464649       1 shared_informer.go:318] Caches are synced for ephemeral
I1120 04:38:43.466365       1 shared_informer.go:318] Caches are synced for PVC protection
I1120 04:38:43.659255       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1120 04:38:43.662848       1 shared_informer.go:318] Caches are synced for endpoint_slice
I1120 04:38:43.787305       1 shared_informer.go:318] Caches are synced for ReplicaSet
I1120 04:38:43.842791       1 shared_informer.go:318] Caches are synced for node
I1120 04:38:43.843031       1 range_allocator.go:174] "Sending events to api server"
I1120 04:38:44.028443       1 shared_informer.go:318] Caches are synced for expand
I1120 04:38:44.080748       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I1120 04:38:44.104218       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1120 04:38:44.763136       1 shared_informer.go:318] Caches are synced for ReplicationController
I1120 04:38:44.148162       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I1120 04:38:45.158266       1 shared_informer.go:318] Caches are synced for cronjob
I1120 04:38:44.498370       1 shared_informer.go:318] Caches are synced for HPA
I1120 04:38:45.165880       1 shared_informer.go:318] Caches are synced for job
I1120 04:38:44.516838       1 shared_informer.go:318] Caches are synced for attach detach
I1120 04:38:44.566700       1 shared_informer.go:318] Caches are synced for persistent volume
I1120 04:38:44.567022       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I1120 04:38:44.567977       1 shared_informer.go:318] Caches are synced for PV protection
I1120 04:38:44.568344       1 shared_informer.go:318] Caches are synced for deployment
I1120 04:38:44.568752       1 shared_informer.go:318] Caches are synced for TTL
I1120 04:38:44.568996       1 shared_informer.go:318] Caches are synced for GC
I1120 04:38:44.569337       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I1120 04:38:44.571390       1 range_allocator.go:178] "Starting range CIDR allocator"
I1120 04:38:45.452039       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I1120 04:38:45.452064       1 shared_informer.go:318] Caches are synced for cidrallocator
I1120 04:38:45.470574       1 shared_informer.go:318] Caches are synced for TTL after finished
I1120 04:38:45.793860       1 shared_informer.go:318] Caches are synced for disruption
I1120 04:38:45.793909       1 shared_informer.go:318] Caches are synced for stateful set
I1120 04:38:45.797812       1 shared_informer.go:318] Caches are synced for taint
I1120 04:38:45.856967       1 shared_informer.go:318] Caches are synced for daemon sets
I1120 04:38:45.857042       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I1120 04:38:45.864096       1 shared_informer.go:318] Caches are synced for resource quota
I1120 04:38:45.963721       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1120 04:38:45.994817       1 shared_informer.go:318] Caches are synced for resource quota
I1120 04:38:45.995077       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I1120 04:38:45.995483       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I1120 04:38:45.995667       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I1120 04:38:46.055272       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I1120 04:38:46.055473       1 taint_manager.go:211] "Sending events to api server"
I1120 04:38:47.279465       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
W1120 04:38:47.954755       1 endpointslice_controller.go:297] Error syncing endpoint slices for service "kube-system/kube-dns", retrying. Error: EndpointSlice informer cache is out of date
I1120 04:38:48.258168       1 trace.go:219] Trace[1336975456]: "DeltaFIFO Pop Process" ID:endpoint-controller,Depth:11,Reason:slow event handlers blocking the queue (20-Nov-2023 04:38:47.972) (total time: 284ms):
Trace[1336975456]: [284.149903ms] [284.149903ms] END
I1120 04:38:48.780135       1 shared_informer.go:318] Caches are synced for garbage collector
I1120 04:38:48.790758       1 shared_informer.go:318] Caches are synced for garbage collector
I1120 04:38:48.790795       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"

* 
* ==> kube-proxy [6ad1871c9cd2] <==
* 
* 
* ==> kube-proxy [f984c2fc3064] <==
* I0225 04:17:08.723889       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0225 04:17:08.724082       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I0225 04:17:08.724205       1 server_others.go:554] "Using iptables proxy"
I0225 04:17:08.997978       1 server_others.go:192] "Using iptables Proxier"
I0225 04:17:08.998112       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0225 04:17:08.998275       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0225 04:17:08.998357       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0225 04:17:08.998490       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0225 04:17:09.000397       1 server.go:658] "Version info" version="v1.27.4"
I0225 04:17:09.000561       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0225 04:17:09.005417       1 config.go:97] "Starting endpoint slice config controller"
I0225 04:17:09.006236       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0225 04:17:09.006530       1 config.go:188] "Starting service config controller"
I0225 04:17:09.006656       1 shared_informer.go:311] Waiting for caches to sync for service config
I0225 04:17:09.006664       1 config.go:315] "Starting node config controller"
I0225 04:17:09.006831       1 shared_informer.go:311] Waiting for caches to sync for node config
I0225 04:17:09.107257       1 shared_informer.go:318] Caches are synced for node config
I0225 04:17:09.107329       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0225 04:17:09.107353       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-scheduler [54aa04a99ec2] <==
* I1120 04:37:59.473099       1 serving.go:348] Generated self-signed cert in-memory
W1120 04:38:16.173114       1 authentication.go:368] Error looking up in-cluster authentication configuration: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication": net/http: TLS handshake timeout
W1120 04:38:16.173293       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1120 04:38:16.173348       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1120 04:38:16.840262       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.4"
I1120 04:38:16.840393       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1120 04:38:17.268142       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1120 04:38:17.268441       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1120 04:38:17.368112       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1120 04:38:17.406276       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
W1120 04:38:17.942649       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1120 04:38:18.035333       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1120 04:38:18.037190       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1120 04:38:18.037377       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1120 04:38:18.037743       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1120 04:38:18.038186       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1120 04:38:18.040586       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1120 04:38:18.041473       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1120 04:38:18.262637       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1120 04:38:18.262982       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1120 04:38:18.289694       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [role.rbac.authorization.k8s.io "extension-apiserver-authentication-reader" not found, role.rbac.authorization.k8s.io "system::leader-locking-kube-scheduler" not found]
E1120 04:38:18.289789       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [role.rbac.authorization.k8s.io "extension-apiserver-authentication-reader" not found, role.rbac.authorization.k8s.io "system::leader-locking-kube-scheduler" not found]
W1120 04:38:18.406171       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1120 04:38:18.406438       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1120 04:38:18.406751       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1120 04:38:18.406795       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1120 04:38:18.407004       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1120 04:38:18.412873       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1120 04:38:18.413679       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1120 04:38:18.413962       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1120 04:38:18.462326       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1120 04:38:18.462383       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1120 04:38:18.658527       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1120 04:38:18.658612       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
I1120 04:38:19.958163       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1120 04:39:09.207650       1 secure_serving.go:255] Stopped listening on 127.0.0.1:10259
I1120 04:39:09.207907       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I1120 04:39:09.208318       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E1120 04:39:09.566827       1 scheduling_queue.go:1139] "Error while retrieving next pod from scheduling queue" err="scheduling queue is closed"
E1120 04:39:09.567132       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kube-scheduler [a3dfbbf8c225] <==
* E0225 04:17:00.682643       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0225 04:17:00.681642       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0225 04:17:00.682770       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0225 04:17:00.681918       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0225 04:17:00.683002       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0225 04:17:00.682087       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0225 04:17:00.683167       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0225 04:17:00.682262       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0225 04:17:00.683325       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0225 04:17:00.682429       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0225 04:17:00.683388       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0225 04:17:00.682584       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0225 04:17:00.683450       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0225 04:17:00.686488       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0225 04:17:00.687066       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0225 04:17:00.687520       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0225 04:17:00.687708       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0225 04:17:00.688165       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0225 04:17:00.688330       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0225 04:17:00.688866       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0225 04:17:00.689338       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0225 04:17:00.689953       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0225 04:17:00.690262       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0225 04:17:00.690915       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0225 04:17:00.691192       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0225 04:17:00.691336       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0225 04:17:00.691908       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0225 04:17:00.691462       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E0225 04:17:00.692017       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W0225 04:17:04.300011       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0225 04:17:04.300119       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0225 04:17:04.300456       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0225 04:17:04.300540       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0225 04:17:04.300749       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0225 04:17:04.300826       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0225 04:17:04.300962       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0225 04:17:04.301038       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0225 04:17:04.317558       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0225 04:17:04.317651       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0225 04:17:04.317804       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0225 04:17:04.317826       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0225 04:17:04.317964       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0225 04:17:04.317985       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0225 04:17:04.318122       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0225 04:17:04.318143       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0225 04:17:04.318424       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0225 04:17:04.318444       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0225 04:17:04.318463       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0225 04:17:04.318517       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0225 04:17:04.318536       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0225 04:17:04.318548       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0225 04:17:04.365985       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0225 04:17:04.366345       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0225 04:17:04.366869       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0225 04:17:04.367411       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0225 04:17:04.368086       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0225 04:17:04.368314       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0225 04:17:04.515334       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0225 04:17:04.515593       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I0225 04:17:07.185697       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Feb 25 04:21:13 minikube kubelet[1811]: E0225 04:21:13.246653    1811 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for todoitem, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="todoitem:latest"
Feb 25 04:21:13 minikube kubelet[1811]: E0225 04:21:13.246724    1811 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for todoitem, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="todoitem:latest"
Feb 25 04:21:13 minikube kubelet[1811]: E0225 04:21:13.247142    1811 kuberuntime_manager.go:1212] container &Container{Name:todoitem-cntnr,Image:todoitem,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pdc6r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod todoitem-deployment-849864ff87-j22mf_default(6e999dec-3380-4e50-925b-815ae663bc5a): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for todoitem, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Feb 25 04:21:13 minikube kubelet[1811]: E0225 04:21:13.247528    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for todoitem, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todoitem-deployment-849864ff87-j22mf" podUID=6e999dec-3380-4e50-925b-815ae663bc5a
Feb 25 04:21:17 minikube kubelet[1811]: E0225 04:21:17.396493    1811 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for todoitem, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="todoitem:latest"
Feb 25 04:21:17 minikube kubelet[1811]: E0225 04:21:17.396721    1811 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for todoitem, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="todoitem:latest"
Feb 25 04:21:17 minikube kubelet[1811]: E0225 04:21:17.397094    1811 kuberuntime_manager.go:1212] container &Container{Name:todoitem-cntnr,Image:todoitem,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bzpnd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod todoitem-deployment-849864ff87-mj5fm_default(59c506f7-acd7-42b1-8ece-e4f626cab3aa): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for todoitem, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Feb 25 04:21:17 minikube kubelet[1811]: E0225 04:21:17.397331    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for todoitem, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todoitem-deployment-849864ff87-mj5fm" podUID=59c506f7-acd7-42b1-8ece-e4f626cab3aa
Feb 25 04:21:25 minikube kubelet[1811]: E0225 04:21:25.394128    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-2tqv5" podUID=a13d1f4c-47ab-45c1-a658-69883a9b7c4f
Feb 25 04:21:25 minikube kubelet[1811]: E0225 04:21:25.394783    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-j22mf" podUID=6e999dec-3380-4e50-925b-815ae663bc5a
Feb 25 04:21:28 minikube kubelet[1811]: E0225 04:21:28.578105    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-mj5fm" podUID=59c506f7-acd7-42b1-8ece-e4f626cab3aa
Feb 25 04:21:39 minikube kubelet[1811]: E0225 04:21:39.399482    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-j22mf" podUID=6e999dec-3380-4e50-925b-815ae663bc5a
Feb 25 04:21:39 minikube kubelet[1811]: E0225 04:21:39.400171    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-2tqv5" podUID=a13d1f4c-47ab-45c1-a658-69883a9b7c4f
Feb 25 04:21:43 minikube kubelet[1811]: E0225 04:21:43.390257    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-mj5fm" podUID=59c506f7-acd7-42b1-8ece-e4f626cab3aa
Feb 25 04:21:50 minikube kubelet[1811]: E0225 04:21:50.399269    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-j22mf" podUID=6e999dec-3380-4e50-925b-815ae663bc5a
Feb 25 04:21:53 minikube kubelet[1811]: E0225 04:21:53.394929    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-2tqv5" podUID=a13d1f4c-47ab-45c1-a658-69883a9b7c4f
Feb 25 04:21:54 minikube kubelet[1811]: W0225 04:21:54.417464    1811 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 25 04:21:56 minikube kubelet[1811]: E0225 04:21:56.393427    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-mj5fm" podUID=59c506f7-acd7-42b1-8ece-e4f626cab3aa
Feb 25 04:22:02 minikube kubelet[1811]: E0225 04:22:02.396678    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-j22mf" podUID=6e999dec-3380-4e50-925b-815ae663bc5a
Feb 25 04:22:07 minikube kubelet[1811]: E0225 04:22:07.396319    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-2tqv5" podUID=a13d1f4c-47ab-45c1-a658-69883a9b7c4f
Feb 25 04:22:11 minikube kubelet[1811]: E0225 04:22:11.393429    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-mj5fm" podUID=59c506f7-acd7-42b1-8ece-e4f626cab3aa
Feb 25 04:22:17 minikube kubelet[1811]: E0225 04:22:17.394589    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-j22mf" podUID=6e999dec-3380-4e50-925b-815ae663bc5a
Feb 25 04:22:21 minikube kubelet[1811]: E0225 04:22:21.393939    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-2tqv5" podUID=a13d1f4c-47ab-45c1-a658-69883a9b7c4f
Feb 25 04:22:23 minikube kubelet[1811]: E0225 04:22:23.396264    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-mj5fm" podUID=59c506f7-acd7-42b1-8ece-e4f626cab3aa
Feb 25 04:22:29 minikube kubelet[1811]: E0225 04:22:29.398405    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-j22mf" podUID=6e999dec-3380-4e50-925b-815ae663bc5a
Feb 25 04:22:33 minikube kubelet[1811]: E0225 04:22:33.396546    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-2tqv5" podUID=a13d1f4c-47ab-45c1-a658-69883a9b7c4f
Feb 25 04:22:36 minikube kubelet[1811]: E0225 04:22:36.534803    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-mj5fm" podUID=59c506f7-acd7-42b1-8ece-e4f626cab3aa
Feb 25 04:22:40 minikube kubelet[1811]: E0225 04:22:40.404819    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-j22mf" podUID=6e999dec-3380-4e50-925b-815ae663bc5a
Feb 25 04:22:46 minikube kubelet[1811]: E0225 04:22:46.400968    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-2tqv5" podUID=a13d1f4c-47ab-45c1-a658-69883a9b7c4f
Feb 25 04:22:50 minikube kubelet[1811]: E0225 04:22:50.392124    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-mj5fm" podUID=59c506f7-acd7-42b1-8ece-e4f626cab3aa
Feb 25 04:22:54 minikube kubelet[1811]: E0225 04:22:54.394433    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-j22mf" podUID=6e999dec-3380-4e50-925b-815ae663bc5a
Feb 25 04:22:58 minikube kubelet[1811]: E0225 04:22:58.391566    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-2tqv5" podUID=a13d1f4c-47ab-45c1-a658-69883a9b7c4f
Feb 25 04:23:01 minikube kubelet[1811]: E0225 04:23:01.393637    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-mj5fm" podUID=59c506f7-acd7-42b1-8ece-e4f626cab3aa
Feb 25 04:23:06 minikube kubelet[1811]: E0225 04:23:06.398714    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-j22mf" podUID=6e999dec-3380-4e50-925b-815ae663bc5a
Feb 25 04:23:12 minikube kubelet[1811]: E0225 04:23:12.392738    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-2tqv5" podUID=a13d1f4c-47ab-45c1-a658-69883a9b7c4f
Feb 25 04:23:13 minikube kubelet[1811]: E0225 04:23:13.393776    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-mj5fm" podUID=59c506f7-acd7-42b1-8ece-e4f626cab3aa
Feb 25 04:23:20 minikube kubelet[1811]: E0225 04:23:20.394932    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-j22mf" podUID=6e999dec-3380-4e50-925b-815ae663bc5a
Feb 25 04:23:26 minikube kubelet[1811]: E0225 04:23:26.406491    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-2tqv5" podUID=a13d1f4c-47ab-45c1-a658-69883a9b7c4f
Feb 25 04:23:28 minikube kubelet[1811]: E0225 04:23:28.435443    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-mj5fm" podUID=59c506f7-acd7-42b1-8ece-e4f626cab3aa
Feb 25 04:23:33 minikube kubelet[1811]: E0225 04:23:33.394681    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-j22mf" podUID=6e999dec-3380-4e50-925b-815ae663bc5a
Feb 25 04:23:39 minikube kubelet[1811]: E0225 04:23:39.395528    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-mj5fm" podUID=59c506f7-acd7-42b1-8ece-e4f626cab3aa
Feb 25 04:23:45 minikube kubelet[1811]: E0225 04:23:45.373339    1811 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for todoitem, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="todoitem:latest"
Feb 25 04:23:45 minikube kubelet[1811]: E0225 04:23:45.373457    1811 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for todoitem, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="todoitem:latest"
Feb 25 04:23:45 minikube kubelet[1811]: E0225 04:23:45.373833    1811 kuberuntime_manager.go:1212] container &Container{Name:todoitem-cntnr,Image:todoitem,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dj62w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod todoitem-deployment-849864ff87-2tqv5_default(a13d1f4c-47ab-45c1-a658-69883a9b7c4f): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for todoitem, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Feb 25 04:23:45 minikube kubelet[1811]: E0225 04:23:45.373896    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for todoitem, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todoitem-deployment-849864ff87-2tqv5" podUID=a13d1f4c-47ab-45c1-a658-69883a9b7c4f
Feb 25 04:23:48 minikube kubelet[1811]: E0225 04:23:48.400261    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-j22mf" podUID=6e999dec-3380-4e50-925b-815ae663bc5a
Feb 25 04:23:55 minikube kubelet[1811]: E0225 04:23:55.397424    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-mj5fm" podUID=59c506f7-acd7-42b1-8ece-e4f626cab3aa
Feb 25 04:23:58 minikube kubelet[1811]: E0225 04:23:58.394008    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-2tqv5" podUID=a13d1f4c-47ab-45c1-a658-69883a9b7c4f
Feb 25 04:24:03 minikube kubelet[1811]: E0225 04:24:03.039234    1811 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for todoitem, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="todoitem:latest"
Feb 25 04:24:03 minikube kubelet[1811]: E0225 04:24:03.040115    1811 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for todoitem, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="todoitem:latest"
Feb 25 04:24:03 minikube kubelet[1811]: E0225 04:24:03.041045    1811 kuberuntime_manager.go:1212] container &Container{Name:todoitem-cntnr,Image:todoitem,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pdc6r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod todoitem-deployment-849864ff87-j22mf_default(6e999dec-3380-4e50-925b-815ae663bc5a): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for todoitem, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Feb 25 04:24:03 minikube kubelet[1811]: E0225 04:24:03.041105    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for todoitem, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todoitem-deployment-849864ff87-j22mf" podUID=6e999dec-3380-4e50-925b-815ae663bc5a
Feb 25 04:24:11 minikube kubelet[1811]: E0225 04:24:11.181140    1811 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for todoitem, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="todoitem:latest"
Feb 25 04:24:11 minikube kubelet[1811]: E0225 04:24:11.181300    1811 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for todoitem, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="todoitem:latest"
Feb 25 04:24:11 minikube kubelet[1811]: E0225 04:24:11.181733    1811 kuberuntime_manager.go:1212] container &Container{Name:todoitem-cntnr,Image:todoitem,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bzpnd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod todoitem-deployment-849864ff87-mj5fm_default(59c506f7-acd7-42b1-8ece-e4f626cab3aa): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for todoitem, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Feb 25 04:24:11 minikube kubelet[1811]: E0225 04:24:11.181809    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for todoitem, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/todoitem-deployment-849864ff87-mj5fm" podUID=59c506f7-acd7-42b1-8ece-e4f626cab3aa
Feb 25 04:24:13 minikube kubelet[1811]: E0225 04:24:13.397194    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-2tqv5" podUID=a13d1f4c-47ab-45c1-a658-69883a9b7c4f
Feb 25 04:24:16 minikube kubelet[1811]: E0225 04:24:16.393846    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-j22mf" podUID=6e999dec-3380-4e50-925b-815ae663bc5a
Feb 25 04:24:25 minikube kubelet[1811]: E0225 04:24:25.401764    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-2tqv5" podUID=a13d1f4c-47ab-45c1-a658-69883a9b7c4f
Feb 25 04:24:25 minikube kubelet[1811]: E0225 04:24:25.406795    1811 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"todoitem-cntnr\" with ImagePullBackOff: \"Back-off pulling image \\\"todoitem\\\"\"" pod="default/todoitem-deployment-849864ff87-mj5fm" podUID=59c506f7-acd7-42b1-8ece-e4f626cab3aa

* 
* ==> storage-provisioner [633338e32141] <==
* I0225 04:17:08.186487       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0225 04:17:38.192731       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

* 
* ==> storage-provisioner [eeb7c5b910ac] <==
* I0225 04:17:53.969564       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0225 04:17:53.996516       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0225 04:17:53.997024       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0225 04:18:11.587569       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0225 04:18:11.588532       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_ff795e03-52ea-4c9a-a1a3-f0071e61fe1e!
I0225 04:18:11.589922       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"55209ddc-ced2-4846-84ad-983dbb83b746", APIVersion:"v1", ResourceVersion:"14946", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_ff795e03-52ea-4c9a-a1a3-f0071e61fe1e became leader
I0225 04:18:11.689989       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_ff795e03-52ea-4c9a-a1a3-f0071e61fe1e!

